import os
import requests
from typing import Dict, List, Optional
from pathlib import Path

class ModelManager:
    """
    AIæ¨¡å‹ç®¡ç†å™¨
    - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰éœ€è¦çš„AIæ¨¡å‹
    - æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
    - æä¾›æ¸…æ™°çš„ä¸‹è½½è¿›åº¦æç¤º
    """
    
    def __init__(self, logger_service):
        self.logger = logger_service
        
        # å®šä¹‰é¡¹ç›®éœ€è¦çš„æ‰€æœ‰æ¨¡å‹
        self.required_models = {
            # Whisper ASRæ¨¡å‹ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½)
            "whisper": {
                "models": ["base", "medium", "large-v2"],
                "priority": "base",  # é»˜è®¤ä½¿ç”¨baseæ¨¡å‹
                "description": "è¯­éŸ³è¯†åˆ«æ¨¡å‹",
                "cache_dir": os.path.expanduser("~/.cache/whisper")
            },
            
            # pyannote.audio è¯´è¯äººåˆ†ç¦»æ¨¡å‹
            "pyannote": {
                "models": ["pyannote/speaker-diarization-3.1"],
                "priority": "pyannote/speaker-diarization-3.1",
                "description": "è¯´è¯äººåˆ†ç¦»æ¨¡å‹",
                "cache_dir": os.path.expanduser("~/.cache/huggingface")
            },
            
            # Demucs éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹
            "demucs": {
                "models": ["htdemucs"],
                "priority": "htdemucs", 
                "description": "éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹",
                "cache_dir": os.path.expanduser("~/.cache/torch/hub")
            }
        }
    
    def check_model_availability(self) -> Dict[str, Dict]:
        """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹çš„å¯ç”¨æ€§"""
        status = {}
        
        for model_type, config in self.required_models.items():
            status[model_type] = {
                "available": False,
                "cached_models": [],
                "missing_models": [],
                "priority_model": config["priority"],
                "description": config["description"]
            }
            
            if model_type == "whisper":
                status[model_type] = self._check_whisper_models(config)
            elif model_type == "pyannote":
                status[model_type] = self._check_pyannote_models(config)
            elif model_type == "demucs":
                status[model_type] = self._check_demucs_models(config)
        
        return status
    
    def _check_whisper_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥Whisperæ¨¡å‹"""
        cache_dir = Path(config["cache_dir"])
        cached_models = []
        
        if cache_dir.exists():
            # æ£€æŸ¥å·²ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
            for model_name in config["models"]:
                model_file = cache_dir / f"{model_name}.pt"
                if model_file.exists():
                    cached_models.append(model_name)
        
        missing_models = [m for m in config["models"] if m not in cached_models]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def _check_pyannote_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥pyannote.audioæ¨¡å‹"""
        cached_models = []
        
        # å°è¯•å¯¼å…¥å¹¶æµ‹è¯•pyannote.audio
        try:
            from pyannote.audio import Pipeline
            # å¦‚æœèƒ½å¯¼å…¥Pipelineï¼Œè¯´æ˜pyannote.audioå·²å®‰è£…
            # å†æ£€æŸ¥æ˜¯å¦èƒ½åŠ è½½æ¨¡å‹ï¼ˆè¿™é‡Œæˆ‘ä»¬å‡è®¾å·²ç»ä¸‹è½½è¿‡ï¼‰
            cached_models = config["models"]
            self.logger.log("DEBUG", "pyannote.audioæ¨¡å—å¯ç”¨ï¼Œå‡è®¾æ¨¡å‹å·²ç¼“å­˜")
        except ImportError:
            self.logger.log("DEBUG", "pyannote.audioæ¨¡å—æœªå®‰è£…")
        except Exception as e:
            self.logger.log("DEBUG", f"pyannote.audioæ£€æŸ¥å¼‚å¸¸: {str(e)}")
        
        # å¤‡ç”¨æ£€æŸ¥ï¼šæŸ¥æ‰¾huggingfaceç¼“å­˜ç›®å½•
        if not cached_models:
            cache_dir = Path(config["cache_dir"])
            if cache_dir.exists():
                pyannote_dirs = list(cache_dir.glob("**/pyannote*"))
                speaker_dirs = list(cache_dir.glob("**/speaker-diarization*"))
                if pyannote_dirs or speaker_dirs:
                    cached_models = config["models"]
                    self.logger.log("DEBUG", f"åœ¨ç¼“å­˜ç›®å½•å‘ç°pyannoteæ¨¡å‹æ–‡ä»¶: {len(pyannote_dirs + speaker_dirs)}ä¸ª")
        
        missing_models = [m for m in config["models"] if m not in cached_models]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def _check_demucs_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥Demucsæ¨¡å‹"""
        # Demucsæ¨¡å‹é€šå¸¸åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾å¦‚æœdemucsåŒ…å·²å®‰è£…ï¼Œæ¨¡å‹å°±å¯ç”¨
        try:
            import demucs
            cached_models = config["models"]
            missing_models = []
        except ImportError:
            cached_models = []
            missing_models = config["models"]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def get_recommended_model(self, model_type: str, available_memory_gb: float = 4.0) -> Optional[str]:
        """æ ¹æ®ç³»ç»Ÿèµ„æºæ¨èæœ€ä½³æ¨¡å‹"""
        if model_type not in self.required_models:
            return None
        
        config = self.required_models[model_type]
        
        if model_type == "whisper":
            # æ ¹æ®å†…å­˜æ¨èWhisperæ¨¡å‹
            if available_memory_gb < 2:
                return "base"  # æœ€è½»é‡çº§
            elif available_memory_gb < 4:
                return "medium" if "medium" in config["models"] else "base"
            else:
                return "large-v2"  # æœ€é«˜è´¨é‡
        
        return config["priority"]
    
    def print_model_status(self):
        """æ‰“å°æ¨¡å‹çŠ¶æ€æŠ¥å‘Š"""
        self.logger.log("INFO", "ğŸ“‹ AIæ¨¡å‹çŠ¶æ€æ£€æŸ¥æŠ¥å‘Š")
        self.logger.log("INFO", "=" * 50)
        
        status = self.check_model_availability()
        
        for model_type, info in status.items():
            self.logger.log("INFO", f"ğŸ” {info['description']} ({model_type})")
            
            if info["available"]:
                self.logger.log("INFO", f"   âœ… å·²ç¼“å­˜: {', '.join(info['cached_models'])}")
                self.logger.log("INFO", f"   ğŸ¯ æ¨èä½¿ç”¨: {info['priority_model']}")
            else:
                self.logger.log("WARNING", f"   âŒ æœªæ‰¾åˆ°ç¼“å­˜æ¨¡å‹")
                
            if info["missing_models"]:
                self.logger.log("WARNING", f"   ğŸ“¥ éœ€è¦ä¸‹è½½: {', '.join(info['missing_models'])}")
            
            self.logger.log("INFO", "")
    
    def prepare_models_for_professional_processing(self) -> Dict[str, str]:
        """
        ä¸ºä¸“ä¸šéŸ³é¢‘å¤„ç†å‡†å¤‡æ¨¡å‹
        è¿”å›æ¨èçš„æ¨¡å‹é…ç½®
        """
        self.logger.log("INFO", "ğŸš€ å‡†å¤‡ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹...")
        
        # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
        try:
            import psutil
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            self.logger.log("INFO", f"ğŸ“Š ç³»ç»Ÿå¯ç”¨å†…å­˜: {available_gb:.1f}GB")
        except ImportError:
            available_gb = 4.0  # é»˜è®¤å‡è®¾4GB
            self.logger.log("WARNING", "æ— æ³•æ£€æµ‹ç³»ç»Ÿå†…å­˜ï¼Œå‡è®¾4GBå¯ç”¨")
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        self.print_model_status()
        
        # æ¨èæ¨¡å‹é…ç½®
        recommended_config = {}
        
        for model_type in ["whisper", "pyannote", "demucs"]:
            recommended_model = self.get_recommended_model(model_type, available_gb)
            recommended_config[model_type] = recommended_model
            
            # æ£€æŸ¥æ¨èæ¨¡å‹æ˜¯å¦å¯ç”¨
            status = self.check_model_availability()
            if not status[model_type]["available"]:
                self.logger.log("WARNING", f"âš ï¸  {model_type} æ¨¡å‹éœ€è¦é¦–æ¬¡ä¸‹è½½ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")
        
        self.logger.log("INFO", "ğŸ¯ æ¨èé…ç½®:")
        for model_type, model_name in recommended_config.items():
            self.logger.log("INFO", f"   {model_type}: {model_name}")
        
        return recommended_config
    
    def estimate_download_time(self, model_type: str) -> str:
        """ä¼°ç®—æ¨¡å‹ä¸‹è½½æ—¶é—´"""
        download_estimates = {
            "whisper": {
                "base": "1-2åˆ†é’Ÿ (~39MB)",
                "medium": "3-5åˆ†é’Ÿ (~769MB)", 
                "large-v2": "5-10åˆ†é’Ÿ (~1.5GB)"
            },
            "pyannote": {
                "pyannote/speaker-diarization-3.1": "2-3åˆ†é’Ÿ (~100MB)"
            },
            "demucs": {
                "htdemucs": "3-5åˆ†é’Ÿ (~200MB)"
            }
        }
        
        if model_type in download_estimates:
            return download_estimates[model_type]
        
        return "æœªçŸ¥"