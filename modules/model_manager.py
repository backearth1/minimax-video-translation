import os
import requests
from typing import Dict, List, Optional
from pathlib import Path

class ModelManager:
    """
    AIæ¨¡å‹ç®¡ç†å™¨
    - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰éœ€è¦çš„AIæ¨¡å‹
    - æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
    - æä¾›æ¸…æ™°çš„ä¸‹è½½è¿›åº¦æç¤º
    """
    
    def __init__(self, logger_service):
        self.logger = logger_service
        
        # é¡¹ç›®æ ¹ç›®å½•
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.models_dir = os.path.join(self.project_root, "models")
        
        # å®šä¹‰é¡¹ç›®éœ€è¦çš„æ‰€æœ‰æ¨¡å‹ - ä½¿ç”¨ç»Ÿä¸€çš„é¡¹ç›®æ¨¡å‹ç›®å½•
        self.required_models = {
            # Whisper ASRæ¨¡å‹ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½)
            "whisper": {
                "models": ["base", "medium", "large-v2"],
                "priority": "large-v2",  # ä¸“ä¸šå¤„ç†ä¼˜å…ˆä½¿ç”¨large-v2æ¨¡å‹
                "description": "è¯­éŸ³è¯†åˆ«æ¨¡å‹",
                "cache_dir": os.path.join(self.models_dir, "whisper")
            },
            
            # pyannote.audio è¯´è¯äººåˆ†ç¦»æ¨¡å‹
            "pyannote": {
                "models": ["pyannote/speaker-diarization-3.1"],
                "priority": "pyannote/speaker-diarization-3.1",
                "description": "è¯´è¯äººåˆ†ç¦»æ¨¡å‹",
                "cache_dir": os.path.join(self.models_dir, "pyannote")
            },
            
            # Demucs éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹
            "demucs": {
                "models": ["955717e8-8726e21a.th"],  # ç›´æ¥ä½¿ç”¨æ–‡ä»¶å
                "priority": "955717e8-8726e21a.th", 
                "description": "éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹",
                "cache_dir": os.path.join(self.models_dir, "demucs", "checkpoints")  # ä½¿ç”¨checkpointså­ç›®å½•
            }
        }
    
    def check_model_availability(self) -> Dict[str, Dict]:
        """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹çš„å¯ç”¨æ€§"""
        status = {}
        
        for model_type, config in self.required_models.items():
            status[model_type] = {
                "available": False,
                "cached_models": [],
                "missing_models": [],
                "priority_model": config["priority"],
                "description": config["description"]
            }
            
            if model_type == "whisper":
                status[model_type] = self._check_whisper_models(config)
            elif model_type == "pyannote":
                status[model_type] = self._check_pyannote_models(config)
            elif model_type == "demucs":
                status[model_type] = self._check_demucs_models(config)
        
        return status
    
    def _check_whisper_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥Whisperæ¨¡å‹"""
        cache_dir = Path(config["cache_dir"])
        cached_models = []
        
        if cache_dir.exists():
            # æ£€æŸ¥å·²ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
            for model_name in config["models"]:
                model_file = cache_dir / f"{model_name}.pt"
                if model_file.exists():
                    cached_models.append(model_name)
        
        missing_models = [m for m in config["models"] if m not in cached_models]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def _check_pyannote_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥pyannote.audioæ¨¡å‹"""
        cache_dir = Path(config["cache_dir"])
        cached_models = []
        
        if cache_dir.exists():
            # æ£€æŸ¥å·²ä¸‹è½½çš„æ¨¡å‹ç›®å½•
            for model_name in config["models"]:
                # pyannoteæ¨¡å‹ä»¥ç‰¹æ®Šæ ¼å¼å­˜å‚¨
                model_dir_name = f"models--{model_name.replace('/', '--')}"
                model_dir = cache_dir / model_dir_name
                if model_dir.exists():
                    cached_models.append(model_name)
                    self.logger.log("DEBUG", f"æ‰¾åˆ°pyannoteæ¨¡å‹: {model_dir}")
        
        missing_models = [m for m in config["models"] if m not in cached_models]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def _check_demucs_models(self, config: Dict) -> Dict:
        """æ£€æŸ¥Demucsæ¨¡å‹"""
        cache_dir = Path(config["cache_dir"])
        cached_models = []
        
        if cache_dir.exists():
            # æ£€æŸ¥å·²ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
            for model_name in config["models"]:
                model_file = cache_dir / model_name
                if model_file.exists():
                    cached_models.append(model_name)
        
        missing_models = [m for m in config["models"] if m not in cached_models]
        
        return {
            "available": len(cached_models) > 0,
            "cached_models": cached_models,
            "missing_models": missing_models,
            "priority_model": config["priority"],
            "description": config["description"]
        }
    
    def get_recommended_model(self, model_type: str, available_memory_gb: float = 4.0, prioritize_quality: bool = True) -> Optional[str]:
        """æ ¹æ®è´¨é‡æˆ–ç³»ç»Ÿèµ„æºæ¨èæœ€ä½³æ¨¡å‹"""
        if model_type not in self.required_models:
            return None
        
        config = self.required_models[model_type]
        
        if model_type == "whisper":
            if prioritize_quality:
                # ä¸“ä¸šAIç¿»è¯‘ï¼šä¼˜å…ˆä½¿ç”¨æœ€é«˜è´¨é‡æ¨¡å‹
                if "large-v2" in config["models"]:
                    return "large-v2"  # æœ€é«˜è´¨é‡
                elif "medium" in config["models"]:
                    return "medium"    # ä¸­ç­‰è´¨é‡
                else:
                    return "base"      # åŸºç¡€è´¨é‡
            else:
                # å¿«é€Ÿå¤„ç†ï¼šæ ¹æ®å†…å­˜æ¨è
                if available_memory_gb < 2:
                    return "base"  # æœ€è½»é‡çº§
                elif available_memory_gb < 4:
                    return "medium" if "medium" in config["models"] else "base"
                else:
                    return "large-v2"  # æœ€é«˜è´¨é‡
        
        return config["priority"]
    
    def print_model_status(self):
        """æ‰“å°æ¨¡å‹çŠ¶æ€æŠ¥å‘Š"""
        self.logger.log("INFO", "ğŸ“‹ AIæ¨¡å‹çŠ¶æ€æ£€æŸ¥æŠ¥å‘Š")
        self.logger.log("INFO", "=" * 50)
        
        status = self.check_model_availability()
        
        for model_type, info in status.items():
            self.logger.log("INFO", f"ğŸ” {info['description']} ({model_type})")
            
            if info["available"]:
                self.logger.log("INFO", f"   âœ… å·²ç¼“å­˜: {', '.join(info['cached_models'])}")
                self.logger.log("INFO", f"   ğŸ¯ æ¨èä½¿ç”¨: {info['priority_model']}")
            else:
                self.logger.log("WARNING", f"   âŒ æœªæ‰¾åˆ°ç¼“å­˜æ¨¡å‹")
                
            if info["missing_models"]:
                self.logger.log("WARNING", f"   ğŸ“¥ éœ€è¦ä¸‹è½½: {', '.join(info['missing_models'])}")
            
            self.logger.log("INFO", "")
    
    def prepare_models_for_professional_processing(self) -> Dict[str, str]:
        """
        ä¸ºä¸“ä¸šéŸ³é¢‘å¤„ç†å‡†å¤‡æ¨¡å‹
        è¿”å›æ¨èçš„æ¨¡å‹é…ç½®ï¼ˆä¼˜å…ˆè´¨é‡ï¼‰
        """
        self.logger.log("INFO", "ğŸš€ å‡†å¤‡ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹...")
        
        # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
        try:
            import psutil
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            self.logger.log("INFO", f"ğŸ“Š ç³»ç»Ÿå¯ç”¨å†…å­˜: {available_gb:.1f}GB")
        except ImportError:
            available_gb = 4.0  # é»˜è®¤å‡è®¾4GB
            self.logger.log("WARNING", "æ— æ³•æ£€æµ‹ç³»ç»Ÿå†…å­˜ï¼Œå‡è®¾4GBå¯ç”¨")
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        self.print_model_status()
        
        # æ¨èæ¨¡å‹é…ç½®ï¼ˆä¸“ä¸šå¤„ç†ä¼˜å…ˆè´¨é‡ï¼‰
        recommended_config = {}
        
        for model_type in ["whisper", "pyannote", "demucs"]:
            # ä¸“ä¸šå¤„ç†æ¨¡å¼ï¼šprioritize_quality=True
            recommended_model = self.get_recommended_model(model_type, available_gb, prioritize_quality=True)
            recommended_config[model_type] = recommended_model
            
            # æ£€æŸ¥æ¨èæ¨¡å‹æ˜¯å¦å¯ç”¨
            status = self.check_model_availability()
            if not status[model_type]["available"]:
                self.logger.log("WARNING", f"âš ï¸  {model_type} æ¨¡å‹éœ€è¦é¦–æ¬¡ä¸‹è½½ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")
        
        # ç‰¹åˆ«æé†’Whisperæ¨¡å‹é€‰æ‹©
        whisper_model = recommended_config.get("whisper", "base")
        if whisper_model == "large-v2":
            self.logger.log("INFO", "ğŸ¯ ä¸“ä¸šæ¨¡å¼ï¼šä½¿ç”¨Whisper large-v2æ¨¡å‹ç¡®ä¿æœ€ä½³è¯†åˆ«æ•ˆæœ")
        elif whisper_model == "medium":
            self.logger.log("INFO", "ğŸ¯ ä¸“ä¸šæ¨¡å¼ï¼šä½¿ç”¨Whisper mediumæ¨¡å‹å¹³è¡¡æ•ˆæœä¸é€Ÿåº¦")
        else:
            self.logger.log("WARNING", "âš ï¸  ä¸“ä¸šæ¨¡å¼ï¼šä½¿ç”¨Whisper baseæ¨¡å‹ï¼Œè¯†åˆ«æ•ˆæœå¯èƒ½å—é™")
        
        self.logger.log("INFO", "ğŸ¯ ä¸“ä¸šAIå¤„ç†é…ç½®ï¼ˆä¼˜å…ˆè´¨é‡ï¼‰:")
        for model_type, model_name in recommended_config.items():
            self.logger.log("INFO", f"   {model_type}: {model_name}")
        
        return recommended_config
    
    def estimate_download_time(self, model_type: str) -> str:
        """ä¼°ç®—æ¨¡å‹ä¸‹è½½æ—¶é—´"""
        download_estimates = {
            "whisper": {
                "base": "1-2åˆ†é’Ÿ (~39MB)",
                "medium": "3-5åˆ†é’Ÿ (~769MB)", 
                "large-v2": "5-10åˆ†é’Ÿ (~1.5GB)"
            },
            "pyannote": {
                "pyannote/speaker-diarization-3.1": "2-3åˆ†é’Ÿ (~100MB)"
            },
            "demucs": {
                "htdemucs": "3-5åˆ†é’Ÿ (~200MB)"
            }
        }
        
        if model_type in download_estimates:
            return download_estimates[model_type]
        
        return "æœªçŸ¥"