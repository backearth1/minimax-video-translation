import os
import torch
import torchaudio
import whisper_timestamped as whisper
from pyannote.audio import Pipeline
from typing import Dict, Any, List, Tuple
import librosa
import soundfile as sf
import subprocess
import tempfile
from pathlib import Path
from .model_manager import ModelManager

class ProfessionalAudioProcessor:
    """
    ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨
    é›†æˆ Demucs + pyannote.audio + whisper-timestamped
    æä¾›å·¥ä¸šçº§éŸ³é¢‘åˆ†ç¦»å’Œç²¾ç¡®ASRåˆ†å‰²
    """
    
    def __init__(self, logger_service):
        self.logger = logger_service
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager(logger_service)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹ - åªåœ¨å®é™…ä½¿ç”¨æ—¶åŠ è½½
        self.whisper_model = None
        self.diarization_pipeline = None
        self._models_initialized = False
        self.recommended_config = {}
        
        self.logger.log("INFO", "ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆ›å»ºå®Œæˆ (æ¨¡å‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½)")
    
    def check_models_status(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çŠ¶æ€ - ç”¨æˆ·å¯ä¸»åŠ¨è°ƒç”¨"""
        self.logger.log("INFO", "ğŸ” æ£€æŸ¥ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹çŠ¶æ€...")
        self.model_manager.print_model_status()
        
        # æä¾›ä¸‹è½½å»ºè®®
        status = self.model_manager.check_model_availability()
        missing_any = any(not info["available"] for info in status.values())
        
        if missing_any:
            self.logger.log("INFO", "ğŸ’¡ å»ºè®®:")
            self.logger.log("INFO", "   1. é¦–æ¬¡ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ¨¡å‹")
            self.logger.log("INFO", "   2. å»ºè®®åœ¨ç½‘ç»œè‰¯å¥½æ—¶è¿›è¡Œé¦–æ¬¡å¤„ç†")
            self.logger.log("INFO", "   3. æ‰€æœ‰æ¨¡å‹ä»…éœ€ä¸‹è½½ä¸€æ¬¡ï¼Œåç»­ä½¿ç”¨æ— éœ€é‡å¤ä¸‹è½½")
        else:
            self.logger.log("INFO", "âœ… æ‰€æœ‰æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç›´æ¥ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘")
    
    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶å†µ"""
        try:
            import psutil
            
            # è·å–å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            
            self.logger.log("INFO", f"ç³»ç»Ÿå¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            
            # å†…å­˜ä¸è¶³è­¦å‘Š
            if available_gb < 2:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜ä¸è¶³2GBï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            elif available_gb < 4:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹")
                
            return True
            
        except ImportError:
            self.logger.log("WARNING", "æ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æº (psutilæœªå®‰è£…)")
            return True
        except Exception as e:
            self.logger.log("WARNING", f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}")
            return True
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ¨¡å‹"""
        try:
            self.logger.log("INFO", "æ­£åœ¨åˆå§‹åŒ–ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹...")
            
            # 0. æ£€æŸ¥ç³»ç»Ÿèµ„æºå’Œæ¨¡å‹å¯ç”¨æ€§
            if not self._check_system_resources():
                self.logger.log("ERROR", "ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹åˆå§‹åŒ–")
                return
            
            # è·å–æ¨èçš„æ¨¡å‹é…ç½®
            self.recommended_config = self.model_manager.prepare_models_for_professional_processing()
            
            # 1. åˆå§‹åŒ– Whisper-timestamped (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_whisper = self.recommended_config.get("whisper", "base")
                self.logger.log("INFO", f"åŠ è½½ Whisper-timestamped æ¨¡å‹: {recommended_whisper}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                status = self.model_manager.check_model_availability()
                if not status["whisper"]["available"] or recommended_whisper not in status["whisper"]["cached_models"]:
                    estimate = self.model_manager.estimate_download_time("whisper").get(recommended_whisper, "æœªçŸ¥")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ {recommended_whisper} æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # å°è¯•åŠ è½½æ¨èæ¨¡å‹ï¼Œå¤±è´¥åˆ™å›é€€
                model_priority = [recommended_whisper, "base"] if recommended_whisper != "base" else ["base"]
                
                for model_name in model_priority:
                    try:
                        self.logger.log("INFO", f"å°è¯•åŠ è½½ {model_name} æ¨¡å‹...")
                        
                        # æ·»åŠ å†…å­˜æ£€æŸ¥
                        if self.device.type == "cuda":
                            import torch
                            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                        
                        # ä½¿ç”¨é¡¹ç›®æ¨¡å‹ç›®å½•
                        project_model_dir = self.model_manager.models_dir
                        whisper_model_path = os.path.join(project_model_dir, "whisper", f"{model_name}.pt")
                        
                        if os.path.exists(whisper_model_path):
                            # ä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                            self.whisper_model = whisper.load_model(whisper_model_path, device=self.device)
                        else:
                            # å›é€€åˆ°æ ‡å‡†åŠ è½½ï¼ˆä¼šè§¦å‘ä¸‹è½½ï¼‰
                            self.whisper_model = whisper.load_model(model_name, device=self.device)
                        self.logger.log("INFO", f"âœ… Whisper {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        break
                    except Exception as model_err:
                        self.logger.log("WARNING", f"{model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {str(model_err)}")
                        # å°è¯•é‡Šæ”¾å†…å­˜
                        if hasattr(self, 'whisper_model') and self.whisper_model:
                            del self.whisper_model
                            self.whisper_model = None
                        continue
                
                if not self.whisper_model:
                    raise Exception("æ‰€æœ‰ Whisper æ¨¡å‹åŠ è½½å¤±è´¥")
                    
            except Exception as e:
                self.logger.log("ERROR", f"Whisper æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                self.whisper_model = None
            
            # 2. åˆå§‹åŒ– pyannote.audio (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_pyannote = self.recommended_config.get("pyannote", "pyannote/speaker-diarization-3.1")
                self.logger.log("INFO", f"åŠ è½½ pyannote.audio æ¨¡å‹: {recommended_pyannote}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                if not status["pyannote"]["available"]:
                    estimate = self.model_manager.estimate_download_time("pyannote").get(recommended_pyannote, "2-3åˆ†é’Ÿ")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ pyannote.audio æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­çš„HuggingFace token
                auth_token = os.getenv("HUGGINGFACE_TOKEN", None)
                
                # è®¾ç½®HF_HOMEæŒ‡å‘é¡¹ç›®ç›®å½•ï¼Œè®©pyannoteä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                old_hf_home = os.environ.get("HF_HOME", None)
                os.environ["HF_HOME"] = os.path.join(self.model_manager.models_dir, "pyannote")
                
                try:
                    # ä½¿ç”¨æ ‡å‡†æ¨¡å‹ååŠ è½½ï¼ˆä¼šä»HF_HOMEæŸ¥æ‰¾ï¼‰
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        recommended_pyannote,
                        use_auth_token=auth_token
                    )
                    
                    self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                    self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as load_err:
                    self.logger.log("WARNING", f"pyannote.audio æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_err)}")
                    # å¦‚æœtokenæœ‰é—®é¢˜ï¼Œå°è¯•æ— tokenåŠ è½½
                    if "token" in str(load_err).lower() or "unauthorized" in str(load_err).lower():
                        self.logger.log("INFO", "å°è¯•æ— tokenåŠ è½½pyannote.audio...")
                        try:
                            self.diarization_pipeline = Pipeline.from_pretrained(recommended_pyannote)
                            self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                            self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ(æ— token)")
                        except Exception as e2:
                            self.logger.log("ERROR", f"æ— tokenåŠ è½½ä¹Ÿå¤±è´¥: {str(e2)}")
                            raise load_err
                finally:
                    # æ¢å¤åŸå§‹HF_HOMEç¯å¢ƒå˜é‡
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
                    
            except Exception as e:
                self.logger.log("ERROR", f"pyannote.audio åŠ è½½å¤±è´¥: {str(e)}")
                self.diarization_pipeline = None
                
                # ç¡®ä¿æ¢å¤ç¯å¢ƒå˜é‡
                if 'old_hf_home' in locals():
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
            
            self.logger.log("INFO", f"ğŸš€ ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
            
        except Exception as e:
            self.logger.log("ERROR", f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def process_audio_professionally(self, audio_path: str, source_language: str = "zh", project_data=None) -> Dict[str, Any]:
        """
        ä¸“ä¸šéŸ³é¢‘å¤„ç†ä¸»æµç¨‹
        
        Args:
            audio_path: åŸå§‹éŸ³é¢‘è·¯å¾„
            source_language: æºè¯­è¨€ä»£ç 
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            self.logger.log("INFO", "ğŸµ å¼€å§‹ä¸“ä¸šéŸ³é¢‘å¤„ç†æµç¨‹...")
            
            # é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–æ¨¡å‹
            if not self._models_initialized:
                self._initialize_models()
                self._models_initialized = True
            
            # æ­¥éª¤1: Demucs éŸ³é¢‘æºåˆ†ç¦»
            separation_result = self._separate_audio_sources(audio_path)
            if not separation_result["success"]:
                return separation_result
            
            vocals_path = separation_result["vocals_path"]
            background_path = separation_result["background_path"]
            
            # ç«‹å³æ›´æ–°project_dataä»¥ä¾¿å‰ç«¯é¢„è§ˆ
            if project_data:
                project_data.vocals_audio_path = vocals_path
                project_data.background_audio_path = background_path
                project_data.set_processing_status("processing", "ğŸµ éŸ³é¢‘åˆ†ç¦»å®Œæˆï¼Œå¼€å§‹è¯´è¯äººåˆ†æ...", 30)
                self.logger.log("INFO", "âœ… Demucsåˆ†ç¦»å®Œæˆï¼ŒéŸ³é¢‘é¢„è§ˆå·²æ›´æ–°")
            
            # æ­¥éª¤2: æ™ºèƒ½è¯­éŸ³è¯†åˆ«+VADåˆ‡åˆ† (30% -> 60%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ—£ï¸ æ™ºèƒ½è¯­éŸ³è¯†åˆ«å’Œåˆ‡åˆ†...", 30)
            self.logger.log("INFO", "ğŸ“ å¼€å§‹æ™ºèƒ½è¯­éŸ³è¯†åˆ«...")
            word_timestamps = self._transcribe_with_timestamps(vocals_path, source_language)
            
            # æ­¥éª¤3: åŸºäºåœé¡¿çš„è¯­ä¹‰åˆ‡åˆ† (60% -> 70%)
            if project_data:
                project_data.set_processing_status("processing", "âœ‚ï¸ æ™ºèƒ½è¯­ä¹‰ç‰‡æ®µåˆ‡åˆ†...", 60)
            self.logger.log("INFO", "âœ‚ï¸ åˆ›å»ºè¯­ä¹‰ç‰‡æ®µ...")
            semantic_segments = self._create_semantic_segments(word_timestamps)
            
            # æ­¥éª¤4: è¯´è¯äººåˆ†ç¦»åˆ†æ (70% -> 80%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ­ åˆ†æè¯´è¯äººåˆ†å¸ƒ...", 70)
            self.logger.log("INFO", "ğŸ“Š å¼€å§‹è¯´è¯äººåˆ†ç¦»åˆ†æ...")
            speaker_segments = self._analyze_speakers(vocals_path)
            
            # æ­¥éª¤5: ç‰‡æ®µçº§è¯´è¯äººæ£€æµ‹ (80% -> 90%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ” æ£€æµ‹å¤šè¯´è¯äººç‰‡æ®µ...", 80)
            self.logger.log("INFO", "ğŸ” æ£€æµ‹å¤šè¯´è¯äººç‰‡æ®µ...")
            aligned_segments = self._detect_multi_speaker_segments(semantic_segments, speaker_segments)
            
            # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç‰‡æ®µ
            final_segments = self._generate_audio_segments(vocals_path, aligned_segments)
            
            self.logger.log("INFO", f"âœ… ä¸“ä¸šéŸ³é¢‘å¤„ç†å®Œæˆ: {len(final_segments)}ä¸ªç²¾ç¡®ç‰‡æ®µ")
            
            return {
                "success": True,
                "vocals_path": vocals_path,
                "background_path": background_path,
                "segments": final_segments,
                "total_segments": len(final_segments)
            }
            
        except Exception as e:
            error_msg = f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}"
            self.logger.log("ERROR", error_msg)
            return {"success": False, "error": error_msg}
    
    def _separate_audio_sources(self, audio_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»"""
        try:
            self.logger.log("INFO", "ğŸ¼ ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»...")
            
            # æ£€æŸ¥Demucsæ¨¡å‹çŠ¶æ€
            status = self.model_manager.check_model_availability()
            if not status["demucs"]["available"]:
                estimate = self.model_manager.estimate_download_time("demucs").get("htdemucs", "3-5åˆ†é’Ÿ")
                self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä½¿ç”¨ Demucsï¼Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = "./temp/demucs_output"
            os.makedirs(output_dir, exist_ok=True)
            
            # è¿è¡Œ Demucs åˆ†ç¦» (ä½¿ç”¨UVç¯å¢ƒï¼ŒæŒ‡å®šé¡¹ç›®æ¨¡å‹)
            # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å‘é¡¹ç›®æ¨¡å‹ç›®å½•
            env = os.environ.copy()
            env["TORCH_HOME"] = os.path.join(self.model_manager.models_dir, "demucs")
            
            cmd = [
                "uv", "run", "python", "-m", "demucs.separate",
                "-n", "htdemucs",  # ä½¿ç”¨é«˜è´¨é‡htdemucsæ¨¡å‹
                "--mp3",  # è¾“å‡ºMP3æ ¼å¼
                "--mp3-bitrate", "320",  # é«˜è´¨é‡
                "-o", output_dir,
                audio_path
            ]
            
            # å¯¹äº32ç§’éŸ³é¢‘ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’(10åˆ†é’Ÿ)
            self.logger.log("INFO", f"æ‰§è¡ŒDemucså‘½ä»¤: {' '.join(cmd)}")
            self.logger.log("INFO", f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {env['TORCH_HOME']}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=env)
            
            if result.returncode != 0:
                return {"success": False, "error": f"Demucs åˆ†ç¦»å¤±è´¥: {result.stderr}"}
            
            # æŸ¥æ‰¾åˆ†ç¦»åçš„æ–‡ä»¶
            audio_name = Path(audio_path).stem
            demucs_subdir = os.path.join(output_dir, "htdemucs", audio_name)
            
            vocals_path = os.path.join(demucs_subdir, "vocals.mp3")
            background_paths = [
                os.path.join(demucs_subdir, "drums.mp3"),
                os.path.join(demucs_subdir, "bass.mp3"), 
                os.path.join(demucs_subdir, "other.mp3")
            ]
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(vocals_path):
                return {"success": False, "error": "Demucs äººå£°åˆ†ç¦»æ–‡ä»¶æœªç”Ÿæˆ"}
            
            # åˆå¹¶éäººå£°éƒ¨åˆ†ä½œä¸ºèƒŒæ™¯éŸ³
            background_path = os.path.join(output_dir, f"{audio_name}_background.wav")
            background_success = self._merge_background_tracks(background_paths, background_path)
            
            if not background_success:
                self.logger.log("WARNING", "èƒŒæ™¯éŸ³è½¨åˆå¹¶å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºèƒŒæ™¯éŸ³")
                background_path = None
            
            # è½¬æ¢äººå£°ä¸ºWAVæ ¼å¼ç”¨äºåç»­å¤„ç†
            vocals_wav_path = os.path.join(output_dir, f"{audio_name}_vocals.wav")
            self._convert_to_wav(vocals_path, vocals_wav_path)
            
            self.logger.log("INFO", f"âœ… Demucs åˆ†ç¦»å®Œæˆ")
            self.logger.log("INFO", f"   äººå£°: {vocals_wav_path}")
            self.logger.log("INFO", f"   èƒŒæ™¯: {background_path}")
            
            return {
                "success": True,
                "vocals_path": vocals_wav_path,
                "background_path": background_path
            }
            
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Demucs å¤„ç†è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "error": f"Demucs å¤„ç†å¼‚å¸¸: {str(e)}"}
    
    def _merge_background_tracks(self, track_paths: List[str], output_path: str) -> bool:
        """åˆå¹¶èƒŒæ™¯éŸ³è½¨ï¼ˆdrums + bass + otherï¼‰"""
        try:
            existing_tracks = [p for p in track_paths if os.path.exists(p)]
            self.logger.log("INFO", f"æ‰¾åˆ°{len(existing_tracks)}ä¸ªèƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
            
            if not existing_tracks:
                self.logger.log("WARNING", "æ²¡æœ‰æ‰¾åˆ°èƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
                return False
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä½¿ç”¨ ffmpeg æ··åˆæ‰€æœ‰èƒŒæ™¯éŸ³è½¨
            if len(existing_tracks) == 1:
                # åªæœ‰ä¸€ä¸ªè½¨é“ï¼Œç›´æ¥è½¬æ¢æ ¼å¼
                cmd = ["ffmpeg", "-i", existing_tracks[0], "-ac", "2", "-ar", "44100", "-y", output_path]
                self.logger.log("INFO", f"å•è½¨é“è½¬æ¢: {existing_tracks[0]}")
            else:
                # å¤šä¸ªè½¨é“ï¼Œéœ€è¦æ··åˆ
                input_args = []
                for track in existing_tracks:
                    input_args.extend(["-i", track])
                    self.logger.log("INFO", f"æ·»åŠ èƒŒæ™¯éŸ³è½¨: {track}")
                
                # ä¿®å¤filter_complexè¯­æ³•
                filter_inputs = "".join([f"[{i}:a]" for i in range(len(existing_tracks))])
                filter_complex = f"{filter_inputs}amix=inputs={len(existing_tracks)}:duration=longest"
                
                cmd = ["ffmpeg"] + input_args + [
                    "-filter_complex", filter_complex,
                    "-ac", "2", "-ar", "44100",
                    "-y", output_path
                ]
            
            self.logger.log("INFO", f"æ‰§è¡Œffmpegå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, timeout=120, text=True)
            
            if result.returncode == 0:
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    self.logger.log("INFO", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶æˆåŠŸ: {output_path} ({file_size} bytes)")
                    return True
                else:
                    self.logger.log("ERROR", "ffmpegæˆåŠŸä½†èƒŒæ™¯éŸ³æ–‡ä»¶æœªç”Ÿæˆ")
                    return False
            else:
                self.logger.log("ERROR", f"ffmpegå¤±è´¥ (è¿”å›ç : {result.returncode})")
                self.logger.log("ERROR", f"stderr: {result.stderr}")
                self.logger.log("ERROR", f"stdout: {result.stdout}")
                return False
            
        except subprocess.TimeoutExpired:
            self.logger.log("ERROR", "èƒŒæ™¯éŸ³è½¨åˆå¹¶è¶…æ—¶")
            return False
        except Exception as e:
            self.logger.log("ERROR", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶å¼‚å¸¸: {str(e)}")
            return False
    
    def _convert_to_wav(self, input_path: str, output_path: str):
        """è½¬æ¢éŸ³é¢‘ä¸ºWAVæ ¼å¼"""
        try:
            cmd = [
                "ffmpeg", "-i", input_path,
                "-acodec", "pcm_s16le",
                "-ar", "16000", "-ac", "1",
                "-y", output_path
            ]
            subprocess.run(cmd, capture_output=True, timeout=60)
        except Exception as e:
            self.logger.log("WARNING", f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
    
    def _analyze_speakers(self, vocals_path: str) -> List[Dict]:
        """ä½¿ç”¨ pyannote.audio åˆ†æè¯´è¯äºº"""
        try:
            if not self.diarization_pipeline:
                return []
            
            diarization = self.diarization_pipeline(vocals_path)
            
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker,
                    "duration": turn.end - turn.start
                })
            
            self.logger.log("INFO", f"æ£€æµ‹åˆ° {len(set(seg['speaker'] for seg in speaker_segments))} ä¸ªè¯´è¯äºº")
            return speaker_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººåˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def _transcribe_with_timestamps(self, vocals_path: str, language: str) -> Dict:
        """ä½¿ç”¨ whisper-timestamped è¿›è¡Œå­—çº§åˆ«è½¬å½•"""
        try:
            if not self.whisper_model:
                return {}
            
            # æ˜ å°„è¯­è¨€ä»£ç 
            lang_map = {
                "ä¸­æ–‡": "zh", "è‹±è¯­": "en", "æ—¥è¯­": "ja", "éŸ©è¯­": "ko",
                "æ³•è¯­": "fr", "å¾·è¯­": "de", "è¥¿ç­ç‰™è¯­": "es"
            }
            whisper_lang = lang_map.get(language, "zh")
            
            result = whisper.transcribe(
                self.whisper_model,
                vocals_path,
                language=whisper_lang,
                vad=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼Œå‡å°‘å¹»è§‰
                compute_word_confidence=True,  # è®¡ç®—è¯æ±‡ç½®ä¿¡åº¦
                refine_whisper_precision=0.5,  # ä¼˜åŒ–æ—¶é—´æˆ³ç²¾åº¦åˆ°0.5ç§’
                min_word_duration=0.02,  # æœ€å°è¯æ±‡æŒç»­æ—¶é—´20ms
                remove_empty_words=True,  # ç§»é™¤å¯èƒ½çš„å¹»è§‰ç©ºè¯
                detect_disfluencies=False,  # æš‚æ—¶å…³é—­ä¸æµç•…æ£€æµ‹
                trust_whisper_timestamps=True  # ä¿¡ä»»Whisperçš„æ—¶é—´æˆ³ä½œä¸ºåŸºç¡€
            )
            
            self.logger.log("INFO", f"Whisper è¯†åˆ«å®Œæˆ: {len(result.get('segments', []))} ä¸ªæ®µè½")
            return result
            
        except Exception as e:
            self.logger.log("ERROR", f"Whisper è½¬å½•å¤±è´¥: {str(e)}")
            return {}
    
    def _detect_multi_speaker_segments(self, semantic_segments: List[Dict], speaker_segments: List[Dict]) -> List[Dict]:
        """ä¸ºæ¯ä¸ªè¯­ä¹‰ç‰‡æ®µæ£€æµ‹è¯´è¯äººä¿¡æ¯"""
        try:
            enhanced_segments = []
            
            self.logger.log("DEBUG", f"è¯­ä¹‰ç‰‡æ®µæ•°é‡: {len(semantic_segments)}")
            self.logger.log("DEBUG", f"è¯´è¯äººç‰‡æ®µæ•°é‡: {len(speaker_segments)}")
            
            for i, segment in enumerate(semantic_segments):
                segment_start = segment["start"]
                segment_end = segment["end"]
                
                # åˆ†ææ­¤ç‰‡æ®µå†…çš„è¯´è¯äººåˆ†å¸ƒ
                speakers_in_segment = self._analyze_speakers_in_segment(
                    segment_start, segment_end, speaker_segments
                )
                
                # å¢å¼ºç‰‡æ®µä¿¡æ¯
                enhanced_segment = segment.copy()
                enhanced_segment.update({
                    "speakers": speakers_in_segment["speakers"],
                    "primary_speaker": speakers_in_segment["primary_speaker"],
                    "speaker_count": len(speakers_in_segment["speakers"]),
                    "multi_speaker": len(speakers_in_segment["speakers"]) > 1,
                    "speaker_confidence": speakers_in_segment["confidence"],
                    "segment_id": i + 1
                })
                
                enhanced_segments.append(enhanced_segment)
                
                # è®°å½•å¤šè¯´è¯äººç‰‡æ®µ
                if enhanced_segment["multi_speaker"]:
                    self.logger.log("INFO", f"ğŸ” ç‰‡æ®µ{i+1}æ£€æµ‹åˆ°å¤šè¯´è¯äºº: {enhanced_segment['speakers']}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            multi_speaker_count = sum(1 for seg in enhanced_segments if seg["multi_speaker"])
            self.logger.log("INFO", f"ğŸ” å¤šè¯´è¯äººæ£€æµ‹å®Œæˆ: {multi_speaker_count}/{len(enhanced_segments)} ä¸ªç‰‡æ®µåŒ…å«å¤šè¯´è¯äºº")
            
            return enhanced_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"å¤šè¯´è¯äººæ£€æµ‹å¤±è´¥: {str(e)}")
            return semantic_segments  # è¿”å›åŸå§‹ç‰‡æ®µä½œä¸ºåå¤‡
    
    def _analyze_speakers_in_segment(self, segment_start: float, segment_end: float, 
                                   speaker_segments: List[Dict]) -> Dict:
        """åˆ†æç‰¹å®šæ—¶é—´æ®µå†…çš„è¯´è¯äººåˆ†å¸ƒ"""
        speakers_info = {}
        
        for speaker_seg in speaker_segments:
            speaker = speaker_seg["speaker"]
            spk_start = speaker_seg["start"]
            spk_end = speaker_seg["end"]
            
            # è®¡ç®—é‡å æ—¶é—´
            overlap_start = max(segment_start, spk_start)
            overlap_end = min(segment_end, spk_end)
            
            if overlap_start < overlap_end:  # æœ‰é‡å 
                overlap_duration = overlap_end - overlap_start
                
                if speaker not in speakers_info:
                    speakers_info[speaker] = {
                        "total_duration": 0,
                        "segments": []
                    }
                
                speakers_info[speaker]["total_duration"] += overlap_duration
                speakers_info[speaker]["segments"].append({
                    "start": overlap_start,
                    "end": overlap_end,
                    "duration": overlap_duration
                })
        
        # è®¡ç®—ä¸»è¦è¯´è¯äººå’Œç½®ä¿¡åº¦
        if speakers_info:
            total_duration = segment_end - segment_start
            primary_speaker = max(speakers_info.keys(), 
                                key=lambda s: speakers_info[s]["total_duration"])
            
            primary_duration = speakers_info[primary_speaker]["total_duration"]
            confidence = primary_duration / total_duration if total_duration > 0 else 0.0
            
            return {
                "speakers": list(speakers_info.keys()),
                "primary_speaker": primary_speaker,
                "confidence": confidence,
                "speaker_durations": {s: info["total_duration"] for s, info in speakers_info.items()}
            }
        else:
            return {
                "speakers": ["SPEAKER_UNKNOWN"],
                "primary_speaker": "SPEAKER_UNKNOWN", 
                "confidence": 0.0,
                "speaker_durations": {"SPEAKER_UNKNOWN": segment_end - segment_start}
            }
    
    def _align_speakers_with_words(self, speaker_segments: List[Dict], word_result: Dict) -> List[Dict]:
        """å°†è¯´è¯äººä¿¡æ¯ä¸æ–‡å­—æ—¶é—´æˆ³å¯¹é½ (å·²åºŸå¼ƒï¼Œç”±å¤šè¯´è¯äººæ£€æµ‹æ›¿ä»£)"""
        try:
            aligned_segments = []
            
            # è°ƒè¯•ä¿¡æ¯
            self.logger.log("DEBUG", f"è¯´è¯äººç‰‡æ®µæ•°é‡: {len(speaker_segments)}")
            self.logger.log("DEBUG", f"Whisperç»“æœåŒ…å«segments: {'segments' in word_result if word_result else False}")
            
            if not word_result or "segments" not in word_result:
                self.logger.log("WARNING", "Whisperç»“æœä¸ºç©ºæˆ–æ²¡æœ‰segmentså­—æ®µ")
                return []
            
            # ç»Ÿè®¡è¯æ±‡æ•°é‡
            total_words = 0
            for segment in word_result["segments"]:
                if "words" in segment:
                    total_words += len(segment["words"])
            
            self.logger.log("DEBUG", f"Whisperè¯†åˆ«å‡ºæ€»è¯æ±‡æ•°: {total_words}")
            
            for i, segment in enumerate(word_result["segments"]):
                if "words" not in segment:
                    continue
                
                for word_info in segment["words"]:
                    word_start = word_info.get("start", 0)
                    word_end = word_info.get("end", 0)
                    word_text = word_info.get("text", "").strip()  # ä¿®å¤ï¼šä½¿ç”¨'text'è€Œä¸æ˜¯'word'
                    
                    if not word_text:
                        continue
                    
                    # æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº
                    speaker = self._find_speaker_at_time(speaker_segments, word_start, word_end)
                    
                    aligned_segments.append({
                        "start": word_start,
                        "end": word_end,
                        "text": word_text,
                        "speaker": speaker
                    })
            
            self.logger.log("DEBUG", f"å¯¹é½åè¯æ±‡æ•°é‡: {len(aligned_segments)}")
            
            # å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­
            grouped_segments = self._group_consecutive_words(aligned_segments)
            
            self.logger.log("INFO", f"å¯¹é½å®Œæˆ: {len(grouped_segments)} ä¸ªè¯´è¯äººç‰‡æ®µ")
            return grouped_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººæ–‡å­—å¯¹é½å¤±è´¥: {str(e)}")
            return []
    
    def _find_speaker_at_time(self, speaker_segments: List[Dict], start_time: float, end_time: float) -> str:
        """æ ¹æ®æ—¶é—´æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº"""
        word_center = (start_time + end_time) / 2
        
        for segment in speaker_segments:
            if segment["start"] <= word_center <= segment["end"]:
                return segment["speaker"]
        
        # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œæ‰¾æœ€è¿‘çš„
        closest_speaker = "SPEAKER_UNKNOWN"
        min_distance = float('inf')
        
        for segment in speaker_segments:
            seg_center = (segment["start"] + segment["end"]) / 2
            distance = abs(word_center - seg_center)
            if distance < min_distance:
                min_distance = distance
                closest_speaker = segment["speaker"]
        
        return closest_speaker
    
    def _create_semantic_segments(self, word_result: Dict) -> List[Dict]:
        """åŸºäºWhisperæ®µè½è¾¹ç•Œçš„æ™ºèƒ½è¯­ä¹‰åˆ‡åˆ†"""
        try:
            if not word_result or "segments" not in word_result:
                self.logger.log("WARNING", "Whisperç»“æœä¸ºç©ºæˆ–æ²¡æœ‰segmentså­—æ®µ")
                return []
            
            whisper_segments = word_result["segments"]
            self.logger.log("INFO", f"ğŸ“ Whisperæä¾›äº†{len(whisper_segments)}ä¸ªåŸå§‹æ®µè½")
            
            semantic_segments = []
            
            for i, segment in enumerate(whisper_segments):
                # æå–åŸºæœ¬ä¿¡æ¯
                start_time = segment.get("start", 0)
                end_time = segment.get("end", 0)
                text = segment.get("text", "").strip()
                
                if not text or end_time <= start_time:
                    continue
                
                # è®¡ç®—æ®µè½ç»Ÿè®¡ä¿¡æ¯
                word_count = len(segment.get("words", []))
                duration = end_time - start_time
                
                # è®¡ç®—ç½®ä¿¡åº¦
                avg_confidence = 0.0
                if "words" in segment and segment["words"]:
                    confidences = [w.get("confidence", 0.0) for w in segment["words"] if "confidence" in w]
                    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†ï¼ˆä»…å¯¹è¿‡é•¿çš„æ®µè½ï¼‰
                if self._should_split_long_segment(segment, duration):
                    # å¯¹è¿‡é•¿æ®µè½è¿›è¡Œè¯çº§åˆ‡åˆ†
                    sub_segments = self._split_long_segment(segment)
                    semantic_segments.extend(sub_segments)
                    self.logger.log("INFO", f"ğŸ”ª æ®µè½{i+1}è¿‡é•¿({duration:.2f}s)ï¼Œåˆ‡åˆ†ä¸º{len(sub_segments)}ä¸ªå­æ®µè½")
                else:
                    # ä¿æŒåŸå§‹æ®µè½
                    semantic_segments.append({
                        "start": start_time,
                        "end": end_time,
                        "text": text,
                        "word_count": word_count,
                        "avg_confidence": avg_confidence,
                        "duration": duration,
                        "whisper_segment_id": i + 1
                    })
            
            self.logger.log("INFO", f"âœ‚ï¸ æ™ºèƒ½è¯­ä¹‰åˆ‡åˆ†å®Œæˆ: {len(whisper_segments)}ä¸ªåŸå§‹æ®µè½ â†’ {len(semantic_segments)}ä¸ªè¯­ä¹‰ç‰‡æ®µ")
            
            # è®°å½•åˆ‡åˆ†ç»Ÿè®¡ä¿¡æ¯
            if semantic_segments:
                avg_duration = sum(seg["duration"] for seg in semantic_segments) / len(semantic_segments)
                max_duration = max(seg["duration"] for seg in semantic_segments)
                min_duration = min(seg["duration"] for seg in semantic_segments)
                self.logger.log("DEBUG", f"ç‰‡æ®µæ—¶é•¿ç»Ÿè®¡: å¹³å‡{avg_duration:.2f}s, æœ€é•¿{max_duration:.2f}s, æœ€çŸ­{min_duration:.2f}s")
            
            return semantic_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯­ä¹‰åˆ‡åˆ†å¤±è´¥: {str(e)}")
            return []
    
    def _should_split_long_segment(self, segment: Dict, duration: float) -> bool:
        """åˆ¤æ–­æ®µè½æ˜¯å¦è¿‡é•¿éœ€è¦åˆ‡åˆ†"""
        # åªåˆ‡åˆ†æ˜æ˜¾è¿‡é•¿çš„æ®µè½
        if duration > 15.0:  # è¶…è¿‡15ç§’å¿…é¡»åˆ‡åˆ†
            return True
        
        # 10-15ç§’çš„æ®µè½ï¼Œå¦‚æœè¯æ±‡æ•°é‡è¿‡å¤šä¹Ÿåˆ‡åˆ†
        if duration > 10.0:
            word_count = len(segment.get("words", []))
            if word_count > 20:  # è¶…è¿‡20ä¸ªè¯
                return True
        
        return False
    
    def _split_long_segment(self, segment: Dict) -> List[Dict]:
        """åˆ‡åˆ†è¿‡é•¿çš„æ®µè½"""
        if "words" not in segment or not segment["words"]:
            # æ²¡æœ‰è¯çº§ä¿¡æ¯ï¼Œæ— æ³•åˆ‡åˆ†ï¼Œè¿”å›åŸæ®µè½
            return [{
                "start": segment.get("start", 0),
                "end": segment.get("end", 0),
                "text": segment.get("text", "").strip(),
                "word_count": 0,
                "avg_confidence": 0.0,
                "duration": segment.get("end", 0) - segment.get("start", 0)
            }]
        
        words = segment["words"]
        sub_segments = []
        current_segment = {
            "start": words[0].get("start", 0),
            "end": words[0].get("end", 0),
            "text": words[0].get("text", "").strip(),
            "word_count": 1,
            "total_confidence": words[0].get("confidence", 0.0)
        }
        
        for i in range(1, len(words)):
            word = words[i]
            prev_word = words[i-1]
            
            # è®¡ç®—åœé¡¿æ—¶é—´
            pause_duration = word.get("start", 0) - prev_word.get("end", 0)
            current_duration = current_segment["end"] - current_segment["start"]
            
            # åˆ‡åˆ†æ¡ä»¶ï¼šé•¿åœé¡¿æˆ–æ®µè½å·²ç»å¾ˆé•¿
            should_split = (
                pause_duration > 2.0 or  # é•¿åœé¡¿ï¼ˆ2ç§’ï¼‰
                (pause_duration > 1.0 and current_duration > 8.0) or  # ä¸­ç­‰åœé¡¿+é•¿æ®µè½
                current_duration > 12.0  # å¼ºåˆ¶åˆ‡åˆ†è¶…é•¿æ®µè½
            )
            
            if should_split:
                # å®Œæˆå½“å‰æ®µè½
                current_segment["duration"] = current_segment["end"] - current_segment["start"]
                current_segment["avg_confidence"] = current_segment["total_confidence"] / current_segment["word_count"]
                del current_segment["total_confidence"]
                sub_segments.append(current_segment)
                
                # å¼€å§‹æ–°æ®µè½
                current_segment = {
                    "start": word.get("start", 0),
                    "end": word.get("end", 0),
                    "text": word.get("text", "").strip(),
                    "word_count": 1,
                    "total_confidence": word.get("confidence", 0.0)
                }
            else:
                # åˆå¹¶åˆ°å½“å‰æ®µè½
                current_segment["end"] = word.get("end", 0)
                current_segment["text"] += word.get("text", "").strip()
                current_segment["word_count"] += 1
                current_segment["total_confidence"] += word.get("confidence", 0.0)
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
        if current_segment:
            current_segment["duration"] = current_segment["end"] - current_segment["start"]
            current_segment["avg_confidence"] = current_segment["total_confidence"] / current_segment["word_count"]
            del current_segment["total_confidence"]
            sub_segments.append(current_segment)
        
        return sub_segments
    
    
    def _group_consecutive_words(self, word_segments: List[Dict]) -> List[Dict]:
        """å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­ (å·²åºŸå¼ƒï¼Œç”±è¯­ä¹‰åˆ‡åˆ†æ›¿ä»£)"""
        if not word_segments:
            return []
        
        grouped = []
        current_group = {
            "start": word_segments[0]["start"],
            "end": word_segments[0]["end"],
            "text": word_segments[0]["text"],
            "speaker": word_segments[0]["speaker"]
        }
        
        for i in range(1, len(word_segments)):
            word = word_segments[i]
            
            # å¦‚æœæ˜¯ç›¸åŒè¯´è¯äººä¸”æ—¶é—´è¿ç»­ï¼ˆé—´éš”<2ç§’ï¼‰ï¼Œåˆ™åˆå¹¶
            if (word["speaker"] == current_group["speaker"] and 
                word["start"] - current_group["end"] < 2.0):
                
                current_group["end"] = word["end"]
                current_group["text"] += word["text"]
            else:
                # ä¿å­˜å½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„
                grouped.append(current_group.copy())
                current_group = {
                    "start": word["start"],
                    "end": word["end"], 
                    "text": word["text"],
                    "speaker": word["speaker"]
                }
        
        # æ·»åŠ æœ€åä¸€ç»„
        grouped.append(current_group)
        
        return grouped
    
    def _generate_audio_segments(self, vocals_path: str, enhanced_segments: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆæœ€ç»ˆçš„éŸ³é¢‘ç‰‡æ®µï¼ˆå…¼å®¹æ–°çš„è¯­ä¹‰åˆ‡åˆ†æ•°æ®ç»“æ„ï¼‰"""
        try:
            final_segments = []
            
            # åŠ è½½äººå£°éŸ³é¢‘
            y, sr = librosa.load(vocals_path, sr=16000)
            
            for i, segment in enumerate(enhanced_segments):
                start_time = segment["start"]
                end_time = segment["end"]
                text = segment["text"].strip()
                
                # ä½¿ç”¨æ–°çš„è¯´è¯äººä¿¡æ¯
                primary_speaker = segment.get("primary_speaker", "SPEAKER_UNKNOWN")
                speakers = segment.get("speakers", [primary_speaker])
                multi_speaker = segment.get("multi_speaker", False)
                speaker_confidence = segment.get("speaker_confidence", 1.0)
                
                # æå–éŸ³é¢‘ç‰‡æ®µ
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                audio_segment = y[start_sample:end_sample]
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«å¤šè¯´è¯äººæ ‡è¯†ï¼‰
                speaker_label = f"multi_{len(speakers)}" if multi_speaker else primary_speaker
                segment_path = f"./temp/professional_segment_{i+1}_{speaker_label}.wav"
                sf.write(segment_path, audio_segment, sr)
                
                # ç”Ÿæˆå¢å¼ºçš„ç‰‡æ®µä¿¡æ¯
                final_segment = {
                    "sequence": i + 1,
                    "timestamp": f"{start_time:.2f}-{end_time:.2f}",
                    "original_text": text,
                    "original_audio_path": segment_path,
                    "translated_text": "",
                    "translated_audio_path": "",
                    "voice_id": "",
                    "speed": 1.0,
                    
                    # æ–°çš„è¯´è¯äººä¿¡æ¯
                    "primary_speaker": primary_speaker,
                    "speakers": speakers,
                    "speaker_count": len(speakers),
                    "multi_speaker": multi_speaker,
                    "speaker_confidence": speaker_confidence,
                    "speaker_durations": segment.get("speaker_durations", {}),
                    
                    # å…¼å®¹æ€§å­—æ®µ
                    "speaker_id": f"speaker_{primary_speaker}",
                    
                    # è¯­ä¹‰åˆ‡åˆ†ä¿¡æ¯
                    "word_count": segment.get("word_count", 0),
                    "avg_confidence": segment.get("avg_confidence", 0.0),
                    "segment_duration": end_time - start_time
                }
                
                final_segments.append(final_segment)
                
                # è®°å½•å¤šè¯´è¯äººç‰‡æ®µ
                if multi_speaker:
                    self.logger.log("INFO", f"ğŸ¯ ç‰‡æ®µ{i+1}: å¤šè¯´è¯äºº {speakers} (ä¸»è¦: {primary_speaker}, ç½®ä¿¡åº¦: {speaker_confidence:.2f})")
            
            # ç»Ÿè®¡ä¿¡æ¯
            multi_count = sum(1 for seg in final_segments if seg["multi_speaker"])
            avg_duration = sum(seg["segment_duration"] for seg in final_segments) / len(final_segments)
            self.logger.log("INFO", f"ğŸ¯ éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå®Œæˆ: {len(final_segments)}ä¸ªç‰‡æ®µ, {multi_count}ä¸ªå¤šè¯´è¯äºº, å¹³å‡æ—¶é•¿{avg_duration:.2f}ç§’")
            
            return final_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå¤±è´¥: {str(e)}")
            return []