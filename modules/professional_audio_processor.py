import os
import torch
import torchaudio
import whisper_timestamped as whisper
from pyannote.audio import Pipeline
from typing import Dict, Any, List, Tuple
import librosa
import soundfile as sf
import subprocess
import tempfile
from pathlib import Path
from .model_manager import ModelManager

class ProfessionalAudioProcessor:
    """
    ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨
    é›†æˆ Demucs + pyannote.audio + whisper-timestamped
    æä¾›å·¥ä¸šçº§éŸ³é¢‘åˆ†ç¦»å’Œç²¾ç¡®ASRåˆ†å‰²
    """
    
    def __init__(self, logger_service):
        self.logger = logger_service
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager(logger_service)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹ - åªåœ¨å®é™…ä½¿ç”¨æ—¶åŠ è½½
        self.whisper_model = None
        self.diarization_pipeline = None
        self._models_initialized = False
        self.recommended_config = {}
        
        self.logger.log("INFO", "ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆ›å»ºå®Œæˆ (æ¨¡å‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½)")
    
    def check_models_status(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çŠ¶æ€ - ç”¨æˆ·å¯ä¸»åŠ¨è°ƒç”¨"""
        self.logger.log("INFO", "ğŸ” æ£€æŸ¥ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹çŠ¶æ€...")
        self.model_manager.print_model_status()
        
        # æä¾›ä¸‹è½½å»ºè®®
        status = self.model_manager.check_model_availability()
        missing_any = any(not info["available"] for info in status.values())
        
        if missing_any:
            self.logger.log("INFO", "ğŸ’¡ å»ºè®®:")
            self.logger.log("INFO", "   1. é¦–æ¬¡ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ¨¡å‹")
            self.logger.log("INFO", "   2. å»ºè®®åœ¨ç½‘ç»œè‰¯å¥½æ—¶è¿›è¡Œé¦–æ¬¡å¤„ç†")
            self.logger.log("INFO", "   3. æ‰€æœ‰æ¨¡å‹ä»…éœ€ä¸‹è½½ä¸€æ¬¡ï¼Œåç»­ä½¿ç”¨æ— éœ€é‡å¤ä¸‹è½½")
        else:
            self.logger.log("INFO", "âœ… æ‰€æœ‰æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç›´æ¥ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘")
    
    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶å†µ"""
        try:
            import psutil
            
            # è·å–å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            
            self.logger.log("INFO", f"ç³»ç»Ÿå¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            
            # å†…å­˜ä¸è¶³è­¦å‘Š
            if available_gb < 2:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜ä¸è¶³2GBï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            elif available_gb < 4:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹")
                
            return True
            
        except ImportError:
            self.logger.log("WARNING", "æ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æº (psutilæœªå®‰è£…)")
            return True
        except Exception as e:
            self.logger.log("WARNING", f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}")
            return True
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ¨¡å‹"""
        try:
            self.logger.log("INFO", "æ­£åœ¨åˆå§‹åŒ–ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹...")
            
            # 0. æ£€æŸ¥ç³»ç»Ÿèµ„æºå’Œæ¨¡å‹å¯ç”¨æ€§
            if not self._check_system_resources():
                self.logger.log("ERROR", "ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹åˆå§‹åŒ–")
                return
            
            # è·å–æ¨èçš„æ¨¡å‹é…ç½®
            self.recommended_config = self.model_manager.prepare_models_for_professional_processing()
            
            # 1. åˆå§‹åŒ– Whisper-timestamped (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_whisper = self.recommended_config.get("whisper", "base")
                self.logger.log("INFO", f"åŠ è½½ Whisper-timestamped æ¨¡å‹: {recommended_whisper}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                status = self.model_manager.check_model_availability()
                if not status["whisper"]["available"] or recommended_whisper not in status["whisper"]["cached_models"]:
                    estimate = self.model_manager.estimate_download_time("whisper").get(recommended_whisper, "æœªçŸ¥")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ {recommended_whisper} æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # å°è¯•åŠ è½½æ¨èæ¨¡å‹ï¼Œå¤±è´¥åˆ™å›é€€
                model_priority = [recommended_whisper, "base"] if recommended_whisper != "base" else ["base"]
                
                for model_name in model_priority:
                    try:
                        self.logger.log("INFO", f"å°è¯•åŠ è½½ {model_name} æ¨¡å‹...")
                        
                        # æ·»åŠ å†…å­˜æ£€æŸ¥
                        if self.device.type == "cuda":
                            import torch
                            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                        
                        # ä½¿ç”¨é¡¹ç›®æ¨¡å‹ç›®å½•
                        project_model_dir = self.model_manager.models_dir
                        whisper_model_path = os.path.join(project_model_dir, "whisper", f"{model_name}.pt")
                        
                        if os.path.exists(whisper_model_path):
                            # ä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                            self.whisper_model = whisper.load_model(whisper_model_path, device=self.device)
                        else:
                            # å›é€€åˆ°æ ‡å‡†åŠ è½½ï¼ˆä¼šè§¦å‘ä¸‹è½½ï¼‰
                            self.whisper_model = whisper.load_model(model_name, device=self.device)
                        self.logger.log("INFO", f"âœ… Whisper {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        break
                    except Exception as model_err:
                        self.logger.log("WARNING", f"{model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {str(model_err)}")
                        # å°è¯•é‡Šæ”¾å†…å­˜
                        if hasattr(self, 'whisper_model') and self.whisper_model:
                            del self.whisper_model
                            self.whisper_model = None
                        continue
                
                if not self.whisper_model:
                    raise Exception("æ‰€æœ‰ Whisper æ¨¡å‹åŠ è½½å¤±è´¥")
                    
            except Exception as e:
                self.logger.log("ERROR", f"Whisper æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                self.whisper_model = None
            
            # 2. åˆå§‹åŒ– pyannote.audio (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_pyannote = self.recommended_config.get("pyannote", "pyannote/speaker-diarization-3.1")
                self.logger.log("INFO", f"åŠ è½½ pyannote.audio æ¨¡å‹: {recommended_pyannote}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                if not status["pyannote"]["available"]:
                    estimate = self.model_manager.estimate_download_time("pyannote").get(recommended_pyannote, "2-3åˆ†é’Ÿ")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ pyannote.audio æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­çš„HuggingFace token
                auth_token = os.getenv("HUGGINGFACE_TOKEN", None)
                
                # è®¾ç½®HF_HOMEæŒ‡å‘é¡¹ç›®ç›®å½•ï¼Œè®©pyannoteä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                old_hf_home = os.environ.get("HF_HOME", None)
                os.environ["HF_HOME"] = os.path.join(self.model_manager.models_dir, "pyannote")
                
                try:
                    # ä½¿ç”¨æ ‡å‡†æ¨¡å‹ååŠ è½½ï¼ˆä¼šä»HF_HOMEæŸ¥æ‰¾ï¼‰
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        recommended_pyannote,
                        use_auth_token=auth_token
                    )
                    
                    self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                    self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as load_err:
                    self.logger.log("WARNING", f"pyannote.audio æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_err)}")
                    # å¦‚æœtokenæœ‰é—®é¢˜ï¼Œå°è¯•æ— tokenåŠ è½½
                    if "token" in str(load_err).lower() or "unauthorized" in str(load_err).lower():
                        self.logger.log("INFO", "å°è¯•æ— tokenåŠ è½½pyannote.audio...")
                        try:
                            self.diarization_pipeline = Pipeline.from_pretrained(recommended_pyannote)
                            self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                            self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ(æ— token)")
                        except Exception as e2:
                            self.logger.log("ERROR", f"æ— tokenåŠ è½½ä¹Ÿå¤±è´¥: {str(e2)}")
                            raise load_err
                finally:
                    # æ¢å¤åŸå§‹HF_HOMEç¯å¢ƒå˜é‡
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
                    
            except Exception as e:
                self.logger.log("ERROR", f"pyannote.audio åŠ è½½å¤±è´¥: {str(e)}")
                self.diarization_pipeline = None
                
                # ç¡®ä¿æ¢å¤ç¯å¢ƒå˜é‡
                if 'old_hf_home' in locals():
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
            
            self.logger.log("INFO", f"ğŸš€ ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
            
        except Exception as e:
            self.logger.log("ERROR", f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def process_audio_professionally(self, audio_path: str, source_language: str = "zh", project_data=None) -> Dict[str, Any]:
        """
        ä¸“ä¸šéŸ³é¢‘å¤„ç†ä¸»æµç¨‹
        
        Args:
            audio_path: åŸå§‹éŸ³é¢‘è·¯å¾„
            source_language: æºè¯­è¨€ä»£ç 
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            self.logger.log("INFO", "ğŸµ å¼€å§‹ä¸“ä¸šéŸ³é¢‘å¤„ç†æµç¨‹...")
            
            # é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–æ¨¡å‹
            if not self._models_initialized:
                self._initialize_models()
                self._models_initialized = True
            
            # æ­¥éª¤1: Demucs éŸ³é¢‘æºåˆ†ç¦»
            separation_result = self._separate_audio_sources(audio_path)
            if not separation_result["success"]:
                return separation_result
            
            vocals_path = separation_result["vocals_path"]
            background_path = separation_result["background_path"]
            
            # ç«‹å³æ›´æ–°project_dataä»¥ä¾¿å‰ç«¯é¢„è§ˆ
            if project_data:
                project_data.vocals_audio_path = vocals_path
                project_data.background_audio_path = background_path
                project_data.set_processing_status("processing", "ğŸµ éŸ³é¢‘åˆ†ç¦»å®Œæˆï¼Œå¼€å§‹è¯´è¯äººåˆ†æ...", 30)
                self.logger.log("INFO", "âœ… Demucsåˆ†ç¦»å®Œæˆï¼ŒéŸ³é¢‘é¢„è§ˆå·²æ›´æ–°")
            
            # æ­¥éª¤2: æ™ºèƒ½è¯­éŸ³è¯†åˆ«+VADåˆ‡åˆ† (30% -> 60%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ—£ï¸ æ™ºèƒ½è¯­éŸ³è¯†åˆ«å’Œåˆ‡åˆ†...", 30)
            self.logger.log("INFO", "ğŸ“ å¼€å§‹æ™ºèƒ½è¯­éŸ³è¯†åˆ«...")
            word_timestamps = self._transcribe_with_timestamps(vocals_path, source_language)
            
            # æ­¥éª¤3: åŸºäºåœé¡¿çš„è¯­ä¹‰åˆ‡åˆ† (60% -> 70%)
            if project_data:
                project_data.set_processing_status("processing", "âœ‚ï¸ æ™ºèƒ½è¯­ä¹‰ç‰‡æ®µåˆ‡åˆ†...", 60)
            self.logger.log("INFO", "âœ‚ï¸ åˆ›å»ºè¯­ä¹‰ç‰‡æ®µ...")
            semantic_segments = self._create_semantic_segments(word_timestamps)
            
            # æ­¥éª¤4: è¯´è¯äººåˆ†ç¦»åˆ†æ (70% -> 80%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ­ åˆ†æè¯´è¯äººåˆ†å¸ƒ...", 70)
            self.logger.log("INFO", "ğŸ“Š å¼€å§‹è¯´è¯äººåˆ†ç¦»åˆ†æ...")
            speaker_segments = self._analyze_speakers(vocals_path)
            
            # æ­¥éª¤5: ç‰‡æ®µçº§è¯´è¯äººæ£€æµ‹ (80% -> 90%)
            if project_data:
                project_data.set_processing_status("processing", "ğŸ” æ£€æµ‹å¤šè¯´è¯äººç‰‡æ®µ...", 80)
            self.logger.log("INFO", "ğŸ” æ£€æµ‹å¤šè¯´è¯äººç‰‡æ®µ...")
            aligned_segments = self._detect_multi_speaker_segments(semantic_segments, speaker_segments)
            
            # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç‰‡æ®µ
            final_segments = self._generate_audio_segments(vocals_path, aligned_segments)
            
            self.logger.log("INFO", f"âœ… ä¸“ä¸šéŸ³é¢‘å¤„ç†å®Œæˆ: {len(final_segments)}ä¸ªç²¾ç¡®ç‰‡æ®µ")
            
            return {
                "success": True,
                "vocals_path": vocals_path,
                "background_path": background_path,
                "segments": final_segments,
                "total_segments": len(final_segments)
            }
            
        except Exception as e:
            error_msg = f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}"
            self.logger.log("ERROR", error_msg)
            return {"success": False, "error": error_msg}
    
    def _separate_audio_sources(self, audio_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»"""
        try:
            self.logger.log("INFO", "ğŸ¼ ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»...")
            
            # æ£€æŸ¥Demucsæ¨¡å‹çŠ¶æ€
            status = self.model_manager.check_model_availability()
            if not status["demucs"]["available"]:
                estimate = self.model_manager.estimate_download_time("demucs").get("htdemucs", "3-5åˆ†é’Ÿ")
                self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä½¿ç”¨ Demucsï¼Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = "./temp/demucs_output"
            os.makedirs(output_dir, exist_ok=True)
            
            # è¿è¡Œ Demucs åˆ†ç¦» (ä½¿ç”¨UVç¯å¢ƒï¼ŒæŒ‡å®šé¡¹ç›®æ¨¡å‹)
            # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å‘é¡¹ç›®æ¨¡å‹ç›®å½•
            env = os.environ.copy()
            env["TORCH_HOME"] = os.path.join(self.model_manager.models_dir, "demucs")
            
            cmd = [
                "uv", "run", "python", "-m", "demucs.separate",
                "-n", "htdemucs",  # ä½¿ç”¨é«˜è´¨é‡htdemucsæ¨¡å‹
                "--mp3",  # è¾“å‡ºMP3æ ¼å¼
                "--mp3-bitrate", "320",  # é«˜è´¨é‡
                "-o", output_dir,
                audio_path
            ]
            
            # å¯¹äº32ç§’éŸ³é¢‘ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’(10åˆ†é’Ÿ)
            self.logger.log("INFO", f"æ‰§è¡ŒDemucså‘½ä»¤: {' '.join(cmd)}")
            self.logger.log("INFO", f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {env['TORCH_HOME']}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=env)
            
            if result.returncode != 0:
                return {"success": False, "error": f"Demucs åˆ†ç¦»å¤±è´¥: {result.stderr}"}
            
            # æŸ¥æ‰¾åˆ†ç¦»åçš„æ–‡ä»¶
            audio_name = Path(audio_path).stem
            demucs_subdir = os.path.join(output_dir, "htdemucs", audio_name)
            
            vocals_path = os.path.join(demucs_subdir, "vocals.mp3")
            background_paths = [
                os.path.join(demucs_subdir, "drums.mp3"),
                os.path.join(demucs_subdir, "bass.mp3"), 
                os.path.join(demucs_subdir, "other.mp3")
            ]
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(vocals_path):
                return {"success": False, "error": "Demucs äººå£°åˆ†ç¦»æ–‡ä»¶æœªç”Ÿæˆ"}
            
            # åˆå¹¶éäººå£°éƒ¨åˆ†ä½œä¸ºèƒŒæ™¯éŸ³
            background_path = os.path.join(output_dir, f"{audio_name}_background.wav")
            background_success = self._merge_background_tracks(background_paths, background_path)
            
            if not background_success:
                self.logger.log("WARNING", "èƒŒæ™¯éŸ³è½¨åˆå¹¶å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºèƒŒæ™¯éŸ³")
                background_path = None
            
            # è½¬æ¢äººå£°ä¸ºWAVæ ¼å¼ç”¨äºåç»­å¤„ç†
            vocals_wav_path = os.path.join(output_dir, f"{audio_name}_vocals.wav")
            self._convert_to_wav(vocals_path, vocals_wav_path)
            
            self.logger.log("INFO", f"âœ… Demucs åˆ†ç¦»å®Œæˆ")
            self.logger.log("INFO", f"   äººå£°: {vocals_wav_path}")
            self.logger.log("INFO", f"   èƒŒæ™¯: {background_path}")
            
            return {
                "success": True,
                "vocals_path": vocals_wav_path,
                "background_path": background_path
            }
            
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Demucs å¤„ç†è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "error": f"Demucs å¤„ç†å¼‚å¸¸: {str(e)}"}
    
    def _merge_background_tracks(self, track_paths: List[str], output_path: str) -> bool:
        """åˆå¹¶èƒŒæ™¯éŸ³è½¨ï¼ˆdrums + bass + otherï¼‰"""
        try:
            existing_tracks = [p for p in track_paths if os.path.exists(p)]
            self.logger.log("INFO", f"æ‰¾åˆ°{len(existing_tracks)}ä¸ªèƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
            
            if not existing_tracks:
                self.logger.log("WARNING", "æ²¡æœ‰æ‰¾åˆ°èƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
                return False
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä½¿ç”¨ ffmpeg æ··åˆæ‰€æœ‰èƒŒæ™¯éŸ³è½¨
            if len(existing_tracks) == 1:
                # åªæœ‰ä¸€ä¸ªè½¨é“ï¼Œç›´æ¥è½¬æ¢æ ¼å¼
                cmd = ["ffmpeg", "-i", existing_tracks[0], "-ac", "2", "-ar", "44100", "-y", output_path]
                self.logger.log("INFO", f"å•è½¨é“è½¬æ¢: {existing_tracks[0]}")
            else:
                # å¤šä¸ªè½¨é“ï¼Œéœ€è¦æ··åˆ
                input_args = []
                for track in existing_tracks:
                    input_args.extend(["-i", track])
                    self.logger.log("INFO", f"æ·»åŠ èƒŒæ™¯éŸ³è½¨: {track}")
                
                # ä¿®å¤filter_complexè¯­æ³•
                filter_inputs = "".join([f"[{i}:a]" for i in range(len(existing_tracks))])
                filter_complex = f"{filter_inputs}amix=inputs={len(existing_tracks)}:duration=longest"
                
                cmd = ["ffmpeg"] + input_args + [
                    "-filter_complex", filter_complex,
                    "-ac", "2", "-ar", "44100",
                    "-y", output_path
                ]
            
            self.logger.log("INFO", f"æ‰§è¡Œffmpegå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, timeout=120, text=True)
            
            if result.returncode == 0:
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    self.logger.log("INFO", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶æˆåŠŸ: {output_path} ({file_size} bytes)")
                    return True
                else:
                    self.logger.log("ERROR", "ffmpegæˆåŠŸä½†èƒŒæ™¯éŸ³æ–‡ä»¶æœªç”Ÿæˆ")
                    return False
            else:
                self.logger.log("ERROR", f"ffmpegå¤±è´¥ (è¿”å›ç : {result.returncode})")
                self.logger.log("ERROR", f"stderr: {result.stderr}")
                self.logger.log("ERROR", f"stdout: {result.stdout}")
                return False
            
        except subprocess.TimeoutExpired:
            self.logger.log("ERROR", "èƒŒæ™¯éŸ³è½¨åˆå¹¶è¶…æ—¶")
            return False
        except Exception as e:
            self.logger.log("ERROR", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶å¼‚å¸¸: {str(e)}")
            return False
    
    def _convert_to_wav(self, input_path: str, output_path: str):
        """è½¬æ¢éŸ³é¢‘ä¸ºWAVæ ¼å¼"""
        try:
            cmd = [
                "ffmpeg", "-i", input_path,
                "-acodec", "pcm_s16le",
                "-ar", "16000", "-ac", "1",
                "-y", output_path
            ]
            subprocess.run(cmd, capture_output=True, timeout=60)
        except Exception as e:
            self.logger.log("WARNING", f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
    
    def _analyze_speakers(self, vocals_path: str) -> List[Dict]:
        """ä½¿ç”¨ pyannote.audio åˆ†æè¯´è¯äºº"""
        try:
            if not self.diarization_pipeline:
                return []
            
            diarization = self.diarization_pipeline(vocals_path)
            
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker,
                    "duration": turn.end - turn.start
                })
            
            self.logger.log("INFO", f"æ£€æµ‹åˆ° {len(set(seg['speaker'] for seg in speaker_segments))} ä¸ªè¯´è¯äºº")
            return speaker_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººåˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def _transcribe_with_timestamps(self, vocals_path: str, language: str) -> Dict:
        """ä½¿ç”¨ whisper-timestamped è¿›è¡Œå­—çº§åˆ«è½¬å½•"""
        try:
            if not self.whisper_model:
                return {}
            
            # æ˜ å°„è¯­è¨€ä»£ç 
            lang_map = {
                "ä¸­æ–‡": "zh", "è‹±è¯­": "en", "æ—¥è¯­": "ja", "éŸ©è¯­": "ko",
                "æ³•è¯­": "fr", "å¾·è¯­": "de", "è¥¿ç­ç‰™è¯­": "es"
            }
            whisper_lang = lang_map.get(language, "zh")
            
            result = whisper.transcribe(
                self.whisper_model,
                vocals_path,
                language=whisper_lang,
                vad=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼Œå‡å°‘å¹»è§‰
                compute_word_confidence=True,  # è®¡ç®—è¯æ±‡ç½®ä¿¡åº¦
                refine_whisper_precision=0.5,  # ä¼˜åŒ–æ—¶é—´æˆ³ç²¾åº¦åˆ°0.5ç§’
                min_word_duration=0.02,  # æœ€å°è¯æ±‡æŒç»­æ—¶é—´20ms
                remove_empty_words=True,  # ç§»é™¤å¯èƒ½çš„å¹»è§‰ç©ºè¯
                detect_disfluencies=False,  # æš‚æ—¶å…³é—­ä¸æµç•…æ£€æµ‹
                trust_whisper_timestamps=True  # ä¿¡ä»»Whisperçš„æ—¶é—´æˆ³ä½œä¸ºåŸºç¡€
            )
            
            self.logger.log("INFO", f"Whisper è¯†åˆ«å®Œæˆ: {len(result.get('segments', []))} ä¸ªæ®µè½")
            return result
            
        except Exception as e:
            self.logger.log("ERROR", f"Whisper è½¬å½•å¤±è´¥: {str(e)}")
            return {}
    
    def _detect_multi_speaker_segments(self, semantic_segments: List[Dict], speaker_segments: List[Dict]) -> List[Dict]:
        """åŸºäºè¯´è¯äººå˜åŒ–æ£€æµ‹å¹¶åˆ‡åˆ†å¤šè¯´è¯äººç‰‡æ®µ"""
        try:
            final_segments = []
            
            self.logger.log("DEBUG", f"è¯­ä¹‰ç‰‡æ®µæ•°é‡: {len(semantic_segments)}")
            self.logger.log("DEBUG", f"è¯´è¯äººç‰‡æ®µæ•°é‡: {len(speaker_segments)}")
            
            for i, segment in enumerate(semantic_segments):
                segment_start = segment["start"]
                segment_end = segment["end"]
                segment_text = segment.get("text", "").strip()
                
                self.logger.log("INFO", f"ğŸ“ åˆ†æç‰‡æ®µ{i+1}: ã€{segment_text}ã€‘ ({segment_start:.2f}s-{segment_end:.2f}s)")
                
                # åˆ†ææ­¤ç‰‡æ®µå†…çš„è¯´è¯äººåˆ†å¸ƒ
                speakers_in_segment = self._analyze_speakers_in_segment(
                    segment_start, segment_end, speaker_segments
                )
                
                # å¦‚æœåªæœ‰ä¸€ä¸ªè¯´è¯äººï¼Œç›´æ¥ä¿ç•™
                if len(speakers_in_segment["speakers"]) <= 1:
                    enhanced_segment = segment.copy()
                    enhanced_segment.update({
                        "speakers": speakers_in_segment["speakers"],
                        "primary_speaker": speakers_in_segment["primary_speaker"],
                        "speaker_count": len(speakers_in_segment["speakers"]),
                        "multi_speaker": False,
                        "speaker_confidence": speakers_in_segment["confidence"],
                        "segment_id": len(final_segments) + 1
                    })
                    final_segments.append(enhanced_segment)
                    self.logger.log("INFO", f"âœ… ç‰‡æ®µ{i+1}: å•è¯´è¯äºº {speakers_in_segment['primary_speaker']} â†’ ä¿æŒåŸæ ·ã€{segment_text}ã€‘")
                
                else:
                    # å¤šè¯´è¯äººç‰‡æ®µï¼Œéœ€è¦åŸºäºè¯´è¯äººå˜åŒ–è¿›ä¸€æ­¥åˆ‡åˆ†
                    self.logger.log("INFO", f"ğŸ” ç‰‡æ®µ{i+1}æ£€æµ‹åˆ°å¤šè¯´è¯äºº {speakers_in_segment['speakers']}ï¼Œå¼€å§‹æ™ºèƒ½åˆ‡åˆ†")
                    self.logger.log("DEBUG", f"åŸå§‹æ–‡æœ¬: ã€{segment_text}ã€‘")
                    
                    sub_segments = self._split_by_speaker_changes(segment, speaker_segments)
                    for j, sub_seg in enumerate(sub_segments):
                        sub_seg["segment_id"] = len(final_segments) + 1
                        final_segments.append(sub_seg)
                        sub_text = sub_seg.get("text", "").strip()
                        self.logger.log("INFO", f"ğŸ”ª ç‰‡æ®µ{i+1}.{j+1}: {sub_seg['primary_speaker']} ({sub_seg['start']:.2f}s-{sub_seg['end']:.2f}s) â†’ ã€{sub_text}ã€‘")
            
            # ç»Ÿè®¡ä¿¡æ¯
            original_multi = sum(1 for i, seg in enumerate(semantic_segments) 
                               if len(self._analyze_speakers_in_segment(seg["start"], seg["end"], speaker_segments)["speakers"]) > 1)
            
            self.logger.log("INFO", f"ğŸ” è¯´è¯äººåˆ‡åˆ†å®Œæˆ: {len(semantic_segments)}ä¸ªåŸå§‹ç‰‡æ®µ â†’ {len(final_segments)}ä¸ªæœ€ç»ˆç‰‡æ®µ")
            self.logger.log("INFO", f"ğŸ” å¤šè¯´è¯äººç‰‡æ®µå¤„ç†: {original_multi}ä¸ªå¤šè¯´è¯äººç‰‡æ®µè¢«åˆ‡åˆ†")
            
            return final_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"å¤šè¯´è¯äººæ£€æµ‹å¤±è´¥: {str(e)}")
            return semantic_segments  # è¿”å›åŸå§‹ç‰‡æ®µä½œä¸ºåå¤‡
    
    def _split_by_speaker_changes(self, segment: Dict, speaker_segments: List[Dict]) -> List[Dict]:
        """æ ¹æ®è¯´è¯äººå˜åŒ–æ™ºèƒ½åˆ‡åˆ†ç‰‡æ®µï¼ˆä¿æŠ¤å¥å­å®Œæ•´æ€§ï¼‰"""
        try:
            words = segment.get("words", [])
            if not words:
                # æ²¡æœ‰è¯çº§ä¿¡æ¯ï¼Œæ— æ³•ç²¾ç¡®åˆ‡åˆ†ï¼Œè¿”å›åŸç‰‡æ®µ
                return [self._create_single_speaker_segment(segment, speaker_segments)]
            
            # é¦–å…ˆä¸ºæ¯ä¸ªè¯åˆ†é…è¯´è¯äºº
            word_speakers = []
            for word in words:
                word_start = word.get("start", 0)
                word_end = word.get("end", 0)
                word_text = word.get("text", "").strip()
                word_speaker = self._find_speaker_at_time(speaker_segments, word_start, word_end)
                
                word_speakers.append({
                    "word": word,
                    "text": word_text,
                    "speaker": word_speaker,
                    "start": word_start,
                    "end": word_end
                })
                
                self.logger.log("DEBUG", f"è¯çº§åˆ†æ: ã€{word_text}ã€‘ â†’ {word_speaker} ({word_start:.2f}s)")
            
            # æŸ¥æ‰¾æ™ºèƒ½åˆ‡åˆ†ç‚¹ï¼ˆè¯´è¯äººå˜åŒ– + æ ‡ç‚¹ä¿æŠ¤ï¼‰
            split_points = self._find_smart_split_points(word_speakers)
            
            if not split_points:
                # æ²¡æœ‰åˆé€‚çš„åˆ‡åˆ†ç‚¹ï¼Œè¿”å›åŸç‰‡æ®µ
                self.logger.log("INFO", f"æœªæ‰¾åˆ°åˆé€‚åˆ‡åˆ†ç‚¹ï¼Œä¿æŒåŸç‰‡æ®µ")
                return [self._create_single_speaker_segment(segment, speaker_segments)]
            
            # æ ¹æ®åˆ‡åˆ†ç‚¹åˆ›å»ºå­ç‰‡æ®µ
            sub_segments = []
            current_start_idx = 0
            
            for split_idx in split_points + [len(word_speakers)]:  # åŠ ä¸Šç»“å°¾
                if split_idx > current_start_idx:
                    segment_words = word_speakers[current_start_idx:split_idx]
                    if segment_words:
                        sub_segment = self._create_segment_from_words(segment_words)
                        sub_segments.append(sub_segment)
                        
                        segment_text = sub_segment["text"]
                        primary_speaker = sub_segment["primary_speaker"]
                        self.logger.log("INFO", f"ğŸ“‹ åˆ›å»ºå­ç‰‡æ®µ: {primary_speaker} â†’ ã€{segment_text}ã€‘")
                
                current_start_idx = split_idx
            
            return sub_segments if sub_segments else [self._create_single_speaker_segment(segment, speaker_segments)]
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººåˆ‡åˆ†å¤±è´¥: {str(e)}")
            return [self._create_single_speaker_segment(segment, speaker_segments)]
    
    def _find_smart_split_points(self, word_speakers: List[Dict]) -> List[int]:
        """æ‰¾åˆ°æ™ºèƒ½åˆ‡åˆ†ç‚¹ï¼ˆè¯´è¯äººå˜åŒ– + æ ‡ç‚¹ä¿æŠ¤ï¼‰"""
        split_points = []
        
        for i in range(1, len(word_speakers)):
            current_word = word_speakers[i]
            prev_word = word_speakers[i-1]
            
            # è¯´è¯äººæ˜¯å¦å˜åŒ–
            speaker_changed = current_word["speaker"] != prev_word["speaker"]
            
            if speaker_changed:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥å®‰å…¨åˆ‡åˆ†ï¼ˆæ ‡ç‚¹ä¿æŠ¤ï¼‰
                can_split = self._can_split_at_position(word_speakers, i)
                
                if can_split:
                    split_points.append(i)
                    self.logger.log("DEBUG", f"åˆ‡åˆ†ç‚¹: ä½ç½®{i}, è¯´è¯äººå˜åŒ– {prev_word['speaker']} â†’ {current_word['speaker']}")
                else:
                    self.logger.log("DEBUG", f"è·³è¿‡åˆ‡åˆ†: ä½ç½®{i}, è¯´è¯äººå˜åŒ–ä½†ä¼šç ´åå¥å­å®Œæ•´æ€§")
        
        return split_points
    
    def _can_split_at_position(self, word_speakers: List[Dict], position: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åœ¨æŒ‡å®šä½ç½®å®‰å…¨åˆ‡åˆ†ï¼ˆä¸¥æ ¼ä¿æŠ¤å¥å­å®Œæ•´æ€§ï¼‰"""
        if position <= 0 or position >= len(word_speakers):
            return False
        
        prev_word = word_speakers[position - 1]
        current_word = word_speakers[position]
        prev_text = prev_word["text"].strip()
        
        # å¼ºåŒ–çš„å¥å­ç»“æŸæ ‡ç‚¹ï¼ˆåªæœ‰çœŸæ­£çš„å¥å­ç»“æŸæ‰å…è®¸åˆ‡åˆ†ï¼‰
        strong_sentence_enders = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ'}
        weak_punctuation = {'ï¼Œ', ',', ';', 'ï¼›', ':', 'ï¼š'}
        
        # ä¼˜å…ˆçº§1: å¼ºå¥å­ç»“æŸæ ‡ç‚¹ + æ˜æ˜¾åœé¡¿
        if prev_text and prev_text[-1] in strong_sentence_enders:
            pause_duration = current_word["start"] - prev_word["end"]
            if pause_duration > 0.5:  # å¥å·åæœ‰0.5ç§’åœé¡¿æ‰åˆ‡åˆ†
                return True
        
        # ä¼˜å…ˆçº§2: éå¸¸æ˜æ˜¾çš„åœé¡¿ï¼ˆ>2ç§’ï¼‰
        pause_duration = current_word["start"] - prev_word["end"]
        if pause_duration > 2.0:  # æé«˜åœé¡¿é˜ˆå€¼åˆ°2ç§’
            return True
        
        # ä¼˜å…ˆçº§3: å¼±æ ‡ç‚¹ + é•¿åœé¡¿ + è¯´è¯äººç½®ä¿¡åº¦æ£€æŸ¥
        if prev_text and prev_text[-1] in weak_punctuation:
            if pause_duration > 1.5:  # å¼±æ ‡ç‚¹éœ€è¦æ›´é•¿åœé¡¿
                # é¢å¤–æ£€æŸ¥ï¼šè¯´è¯äººå˜åŒ–æ˜¯å¦è¶³å¤Ÿæ˜æ˜¾
                if self._is_speaker_change_confident(word_speakers, position):
                    return True
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ä¸­é—´ï¼ˆå¼ºåˆ¶ä¿æŠ¤ï¼‰
        if self._is_in_middle_of_sentence(word_speakers, position):
            self.logger.log("DEBUG", f"æ‹’ç»åˆ‡åˆ†: ä½ç½®{position}å¤„äºå¥å­ä¸­é—´")
            return False
        
        # æ£€æŸ¥æ–‡æœ¬ç‰¹å¾ï¼šæ˜¯å¦åƒæ˜¯è¿ç»­æ— ç©ºæ ¼çš„æ–‡æœ¬
        if self._is_continuous_text(word_speakers, position):
            return False  # è¿ç»­æ–‡æœ¬ä¸åˆ‡åˆ†
        
        return False  # é»˜è®¤ä¸åˆ‡åˆ†ï¼Œæ›´ä¿å®ˆ
    
    def _is_continuous_text(self, word_speakers: List[Dict], position: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­æ— ç©ºæ ¼çš„æ–‡æœ¬ï¼ˆä¸åº”è¯¥åˆ‡åˆ†ï¼‰"""
        # æ£€æŸ¥å‰åå‡ ä¸ªè¯ï¼Œçœ‹æ˜¯å¦éƒ½æ²¡æœ‰ç©ºæ ¼ä¸”æ²¡æœ‰æ ‡ç‚¹
        start_idx = max(0, position - 2)
        end_idx = min(len(word_speakers), position + 2)
        
        continuous_chars = 0
        total_chars = 0
        
        for i in range(start_idx, end_idx):
            word_text = word_speakers[i]["text"].strip()
            if word_text:
                total_chars += len(word_text)
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å­—æ¯ä½†æ²¡æœ‰ç©ºæ ¼
                if word_text.isalnum() and ' ' not in word_text:
                    continuous_chars += len(word_text)
        
        # å¦‚æœå¤§éƒ¨åˆ†æ˜¯è¿ç»­å­—ç¬¦ï¼Œå¯èƒ½æ˜¯è¯†åˆ«é”™è¯¯çš„è¿ç»­æ–‡æœ¬
        if total_chars > 0 and continuous_chars / total_chars > 0.8:
            return True
        
        return False
    
    def _is_speaker_change_confident(self, word_speakers: List[Dict], position: int) -> bool:
        """æ£€æŸ¥è¯´è¯äººå˜åŒ–æ˜¯å¦è¶³å¤Ÿæ˜æ˜¾ï¼ˆé™ä½æ•æ„Ÿåº¦ï¼‰"""
        if position <= 0 or position >= len(word_speakers):
            return False
        
        # æ£€æŸ¥å‰åå‡ ä¸ªè¯çš„è¯´è¯äººä¸€è‡´æ€§
        prev_speaker = word_speakers[position - 1]["speaker"]
        current_speaker = word_speakers[position]["speaker"]
        
        # å‘å‰æ£€æŸ¥2-3ä¸ªè¯ï¼Œç¡®è®¤å‰é¢ç¡®å®æ˜¯åŒä¸€ä¸ªè¯´è¯äºº
        consistent_prev = 0
        for i in range(max(0, position - 3), position):
            if word_speakers[i]["speaker"] == prev_speaker:
                consistent_prev += 1
        
        # å‘åæ£€æŸ¥2-3ä¸ªè¯ï¼Œç¡®è®¤åé¢ä¹Ÿæ˜¯åŒä¸€ä¸ªè¯´è¯äºº
        consistent_next = 0
        for i in range(position, min(len(word_speakers), position + 3)):
            if word_speakers[i]["speaker"] == current_speaker:
                consistent_next += 1
        
        # åªæœ‰å‰åéƒ½æœ‰è¶³å¤Ÿä¸€è‡´æ€§æ‰è®¤ä¸ºæ˜¯å¯ä¿¡çš„è¯´è¯äººå˜åŒ–
        return consistent_prev >= 2 and consistent_next >= 2
    
    def _is_in_middle_of_sentence(self, word_speakers: List[Dict], position: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ä¸­é—´ï¼ˆä¸åº”è¯¥åˆ‡åˆ†ï¼‰"""
        if position <= 0 or position >= len(word_speakers):
            return False
        
        # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„å¥å­å¼€å§‹æˆ–ç»“æŸæ ‡è®°
        sentence_markers = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ'}
        
        # æ£€æŸ¥å‰é¢å‡ ä¸ªè¯ï¼Œçœ‹æ˜¯å¦æœ‰å¥å­ç»“æŸæ ‡è®°
        found_sentence_end = False
        for i in range(position - 1, max(-1, position - 8), -1):  # å‘å‰æ£€æŸ¥æœ€å¤š8ä¸ªè¯
            word_text = word_speakers[i]["text"].strip()
            if word_text and word_text[-1] in sentence_markers:
                found_sentence_end = True
                break
        
        # æ£€æŸ¥åé¢å‡ ä¸ªè¯ï¼Œçœ‹æ˜¯å¦æœ‰å¥å­ç»“æŸæ ‡è®°
        found_sentence_end_after = False
        for i in range(position, min(len(word_speakers), position + 8)):  # å‘åæ£€æŸ¥æœ€å¤š8ä¸ªè¯
            word_text = word_speakers[i]["text"].strip()
            if word_text and word_text[-1] in sentence_markers:
                found_sentence_end_after = True
                break
        
        # å¦‚æœå‰é¢æ²¡æœ‰å¥å­ç»“æŸï¼Œåé¢æœ‰å¥å­ç»“æŸï¼Œåˆ™å½“å‰ä½ç½®åœ¨å¥å­ä¸­é—´
        if not found_sentence_end and found_sentence_end_after:
            return True
        
        return False
    
    def _create_segment_from_words(self, segment_words: List[Dict]) -> Dict:
        """ä»è¯æ±‡åˆ—è¡¨åˆ›å»ºç‰‡æ®µ"""
        if not segment_words:
            return None
        
        start_time = segment_words[0]["start"]
        end_time = segment_words[-1]["end"]
        
        # æ™ºèƒ½æ–‡æœ¬æ‹¼æ¥ï¼ˆå¤„ç†ç©ºæ ¼é—®é¢˜ï¼‰
        text_parts = []
        for word_info in segment_words:
            word_text = word_info["text"].strip()
            if word_text:
                text_parts.append(word_text)
        
        # æ‹¼æ¥æ–‡æœ¬ï¼ˆè‡ªåŠ¨å¤„ç†ç©ºæ ¼ï¼‰
        full_text = self._smart_text_join(text_parts)
        
        # ç»Ÿè®¡è¯´è¯äººåˆ†å¸ƒ
        speaker_counts = {}
        for word_info in segment_words:
            speaker = word_info["speaker"]
            speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1
        
        # æ‰¾åˆ°ä¸»è¦è¯´è¯äºº
        primary_speaker = max(speaker_counts.keys(), key=lambda s: speaker_counts[s])
        
        return {
            "start": start_time,
            "end": end_time,
            "text": full_text,
            "word_count": len(segment_words),
            "avg_confidence": sum(w["word"].get("confidence", 0.0) for w in segment_words) / len(segment_words),
            "duration": end_time - start_time,
            "words": [w["word"] for w in segment_words],
            "speakers": list(speaker_counts.keys()),
            "primary_speaker": primary_speaker,
            "speaker_count": len(speaker_counts),
            "multi_speaker": len(speaker_counts) > 1,
            "speaker_confidence": speaker_counts[primary_speaker] / len(segment_words)
        }
    
    def _smart_text_join(self, text_parts: List[str]) -> str:
        """æ™ºèƒ½æ–‡æœ¬æ‹¼æ¥ï¼ˆå¤„ç†ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰"""
        if not text_parts:
            return ""
        
        result = text_parts[0]
        
        for i in range(1, len(text_parts)):
            current = text_parts[i]
            prev = text_parts[i-1]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç©ºæ ¼
            need_space = True
            
            # å¦‚æœå‰ä¸€ä¸ªè¯ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œæˆ–å½“å‰è¯ä»¥æ ‡ç‚¹å¼€å§‹ï¼Œä¸éœ€è¦ç©ºæ ¼
            if (prev and prev[-1] in '.,;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ') or \
               (current and current[0] in '.,;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ'):
                need_space = False
            
            # å¦‚æœæ˜¯è¿ç»­çš„å­—æ¯æ•°å­—ï¼Œå¯èƒ½éœ€è¦ç©ºæ ¼
            if prev and current and prev[-1].isalnum() and current[0].isalnum():
                need_space = True
            
            if need_space and not prev.endswith(' ') and not current.startswith(' '):
                result += " " + current
            else:
                result += current
        
        return result.strip()
    
    def _create_speaker_segment(self, start: float, end: float, text: str, speaker: str, words: List[Dict]) -> Dict:
        """åˆ›å»ºå•è¯´è¯äººç‰‡æ®µ"""
        duration = end - start
        word_count = len(words)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        avg_confidence = 0.0
        if words:
            confidences = [w.get("confidence", 0.0) for w in words if "confidence" in w]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        return {
            "start": start,
            "end": end,
            "text": text,
            "word_count": word_count,
            "avg_confidence": avg_confidence,
            "duration": duration,
            "words": words,
            "speakers": [speaker],
            "primary_speaker": speaker,
            "speaker_count": 1,
            "multi_speaker": False,
            "speaker_confidence": 1.0
        }
    
    def _create_single_speaker_segment(self, segment: Dict, speaker_segments: List[Dict]) -> Dict:
        """ä¸ºå•è¯´è¯äººç‰‡æ®µåˆ›å»ºå¢å¼ºä¿¡æ¯"""
        speakers_info = self._analyze_speakers_in_segment(
            segment["start"], segment["end"], speaker_segments
        )
        
        enhanced_segment = segment.copy()
        enhanced_segment.update({
            "speakers": speakers_info["speakers"],
            "primary_speaker": speakers_info["primary_speaker"],
            "speaker_count": len(speakers_info["speakers"]),
            "multi_speaker": len(speakers_info["speakers"]) > 1,
            "speaker_confidence": speakers_info["confidence"]
        })
        
        return enhanced_segment
    
    def _analyze_speakers_in_segment(self, segment_start: float, segment_end: float, 
                                   speaker_segments: List[Dict]) -> Dict:
        """åˆ†æç‰¹å®šæ—¶é—´æ®µå†…çš„è¯´è¯äººåˆ†å¸ƒ"""
        speakers_info = {}
        
        for speaker_seg in speaker_segments:
            speaker = speaker_seg["speaker"]
            spk_start = speaker_seg["start"]
            spk_end = speaker_seg["end"]
            
            # è®¡ç®—é‡å æ—¶é—´
            overlap_start = max(segment_start, spk_start)
            overlap_end = min(segment_end, spk_end)
            
            if overlap_start < overlap_end:  # æœ‰é‡å 
                overlap_duration = overlap_end - overlap_start
                
                if speaker not in speakers_info:
                    speakers_info[speaker] = {
                        "total_duration": 0,
                        "segments": []
                    }
                
                speakers_info[speaker]["total_duration"] += overlap_duration
                speakers_info[speaker]["segments"].append({
                    "start": overlap_start,
                    "end": overlap_end,
                    "duration": overlap_duration
                })
        
        # è®¡ç®—ä¸»è¦è¯´è¯äººå’Œç½®ä¿¡åº¦
        if speakers_info:
            total_duration = segment_end - segment_start
            primary_speaker = max(speakers_info.keys(), 
                                key=lambda s: speakers_info[s]["total_duration"])
            
            primary_duration = speakers_info[primary_speaker]["total_duration"]
            confidence = primary_duration / total_duration if total_duration > 0 else 0.0
            
            return {
                "speakers": list(speakers_info.keys()),
                "primary_speaker": primary_speaker,
                "confidence": confidence,
                "speaker_durations": {s: info["total_duration"] for s, info in speakers_info.items()}
            }
        else:
            return {
                "speakers": ["SPEAKER_UNKNOWN"],
                "primary_speaker": "SPEAKER_UNKNOWN", 
                "confidence": 0.0,
                "speaker_durations": {"SPEAKER_UNKNOWN": segment_end - segment_start}
            }
    
    def _align_speakers_with_words(self, speaker_segments: List[Dict], word_result: Dict) -> List[Dict]:
        """å°†è¯´è¯äººä¿¡æ¯ä¸æ–‡å­—æ—¶é—´æˆ³å¯¹é½ (å·²åºŸå¼ƒï¼Œç”±å¤šè¯´è¯äººæ£€æµ‹æ›¿ä»£)"""
        try:
            aligned_segments = []
            
            # è°ƒè¯•ä¿¡æ¯
            self.logger.log("DEBUG", f"è¯´è¯äººç‰‡æ®µæ•°é‡: {len(speaker_segments)}")
            self.logger.log("DEBUG", f"Whisperç»“æœåŒ…å«segments: {'segments' in word_result if word_result else False}")
            
            if not word_result or "segments" not in word_result:
                self.logger.log("WARNING", "Whisperç»“æœä¸ºç©ºæˆ–æ²¡æœ‰segmentså­—æ®µ")
                return []
            
            # ç»Ÿè®¡è¯æ±‡æ•°é‡
            total_words = 0
            for segment in word_result["segments"]:
                if "words" in segment:
                    total_words += len(segment["words"])
            
            self.logger.log("DEBUG", f"Whisperè¯†åˆ«å‡ºæ€»è¯æ±‡æ•°: {total_words}")
            
            for i, segment in enumerate(word_result["segments"]):
                if "words" not in segment:
                    continue
                
                for word_info in segment["words"]:
                    word_start = word_info.get("start", 0)
                    word_end = word_info.get("end", 0)
                    word_text = word_info.get("text", "").strip()  # ä¿®å¤ï¼šä½¿ç”¨'text'è€Œä¸æ˜¯'word'
                    
                    if not word_text:
                        continue
                    
                    # æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº
                    speaker = self._find_speaker_at_time(speaker_segments, word_start, word_end)
                    
                    aligned_segments.append({
                        "start": word_start,
                        "end": word_end,
                        "text": word_text,
                        "speaker": speaker
                    })
            
            self.logger.log("DEBUG", f"å¯¹é½åè¯æ±‡æ•°é‡: {len(aligned_segments)}")
            
            # å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­
            grouped_segments = self._group_consecutive_words(aligned_segments)
            
            self.logger.log("INFO", f"å¯¹é½å®Œæˆ: {len(grouped_segments)} ä¸ªè¯´è¯äººç‰‡æ®µ")
            return grouped_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººæ–‡å­—å¯¹é½å¤±è´¥: {str(e)}")
            return []
    
    def _find_speaker_at_time(self, speaker_segments: List[Dict], start_time: float, end_time: float) -> str:
        """æ ¹æ®æ—¶é—´æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº"""
        word_center = (start_time + end_time) / 2
        
        for segment in speaker_segments:
            if segment["start"] <= word_center <= segment["end"]:
                return segment["speaker"]
        
        # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œæ‰¾æœ€è¿‘çš„
        closest_speaker = "SPEAKER_UNKNOWN"
        min_distance = float('inf')
        
        for segment in speaker_segments:
            seg_center = (segment["start"] + segment["end"]) / 2
            distance = abs(word_center - seg_center)
            if distance < min_distance:
                min_distance = distance
                closest_speaker = segment["speaker"]
        
        return closest_speaker
    
    def _create_semantic_segments(self, word_result: Dict) -> List[Dict]:
        """åŸºäºWhisperæ®µè½è¾¹ç•Œåˆ›å»ºè¯­ä¹‰æ®µè½ï¼ˆä¿æŒåŸå§‹åˆ‡åˆ†ï¼Œä¸ºè¯´è¯äººåˆ†æåšå‡†å¤‡ï¼‰"""
        try:
            if not word_result or "segments" not in word_result:
                self.logger.log("WARNING", "Whisperç»“æœä¸ºç©ºæˆ–æ²¡æœ‰segmentså­—æ®µ")
                return []
            
            whisper_segments = word_result["segments"]
            self.logger.log("INFO", f"ğŸ“ Whisperæä¾›äº†{len(whisper_segments)}ä¸ªåŸå§‹æ®µè½ï¼Œä¿æŒä¸å˜")
            
            semantic_segments = []
            
            for i, segment in enumerate(whisper_segments):
                # æå–åŸºæœ¬ä¿¡æ¯
                start_time = segment.get("start", 0)
                end_time = segment.get("end", 0)
                text = segment.get("text", "").strip()
                
                if not text or end_time <= start_time:
                    continue
                
                # è®¡ç®—æ®µè½ç»Ÿè®¡ä¿¡æ¯
                word_count = len(segment.get("words", []))
                duration = end_time - start_time
                
                # è®¡ç®—ç½®ä¿¡åº¦
                avg_confidence = 0.0
                if "words" in segment and segment["words"]:
                    confidences = [w.get("confidence", 0.0) for w in segment["words"] if "confidence" in w]
                    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
                
                # ä¿æŒåŸå§‹Whisperæ®µè½ï¼Œä¸åšä»»ä½•åˆå¹¶æˆ–åˆ‡åˆ†
                semantic_segments.append({
                    "start": start_time,
                    "end": end_time,
                    "text": text,
                    "word_count": word_count,
                    "avg_confidence": avg_confidence,
                    "duration": duration,
                    "whisper_segment_id": i + 1,
                    "words": segment.get("words", [])  # ä¿ç•™è¯çº§ä¿¡æ¯ç”¨äºè¯´è¯äººåˆ‡åˆ†
                })
            
            self.logger.log("INFO", f"âœ‚ï¸ ä¿æŒWhisperåŸå§‹åˆ‡åˆ†: {len(semantic_segments)}ä¸ªè¯­ä¹‰ç‰‡æ®µ")
            
            # è®°å½•åˆ‡åˆ†ç»Ÿè®¡ä¿¡æ¯
            if semantic_segments:
                avg_duration = sum(seg["duration"] for seg in semantic_segments) / len(semantic_segments)
                max_duration = max(seg["duration"] for seg in semantic_segments)
                min_duration = min(seg["duration"] for seg in semantic_segments)
                self.logger.log("DEBUG", f"ç‰‡æ®µæ—¶é•¿ç»Ÿè®¡: å¹³å‡{avg_duration:.2f}s, æœ€é•¿{max_duration:.2f}s, æœ€çŸ­{min_duration:.2f}s")
            
            return semantic_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯­ä¹‰åˆ‡åˆ†å¤±è´¥: {str(e)}")
            return []
    
    
    
    def _group_consecutive_words(self, word_segments: List[Dict]) -> List[Dict]:
        """å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­ (å·²åºŸå¼ƒï¼Œç”±è¯­ä¹‰åˆ‡åˆ†æ›¿ä»£)"""
        if not word_segments:
            return []
        
        grouped = []
        current_group = {
            "start": word_segments[0]["start"],
            "end": word_segments[0]["end"],
            "text": word_segments[0]["text"],
            "speaker": word_segments[0]["speaker"]
        }
        
        for i in range(1, len(word_segments)):
            word = word_segments[i]
            
            # å¦‚æœæ˜¯ç›¸åŒè¯´è¯äººä¸”æ—¶é—´è¿ç»­ï¼ˆé—´éš”<2ç§’ï¼‰ï¼Œåˆ™åˆå¹¶
            if (word["speaker"] == current_group["speaker"] and 
                word["start"] - current_group["end"] < 2.0):
                
                current_group["end"] = word["end"]
                current_group["text"] += word["text"]
            else:
                # ä¿å­˜å½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„
                grouped.append(current_group.copy())
                current_group = {
                    "start": word["start"],
                    "end": word["end"], 
                    "text": word["text"],
                    "speaker": word["speaker"]
                }
        
        # æ·»åŠ æœ€åä¸€ç»„
        grouped.append(current_group)
        
        return grouped
    
    def _generate_audio_segments(self, vocals_path: str, enhanced_segments: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆæœ€ç»ˆçš„éŸ³é¢‘ç‰‡æ®µï¼ˆå…¼å®¹æ–°çš„è¯­ä¹‰åˆ‡åˆ†æ•°æ®ç»“æ„ï¼‰"""
        try:
            final_segments = []
            
            # åŠ è½½äººå£°éŸ³é¢‘
            y, sr = librosa.load(vocals_path, sr=16000)
            
            for i, segment in enumerate(enhanced_segments):
                start_time = segment["start"]
                end_time = segment["end"]
                text = segment["text"].strip()
                
                # ä½¿ç”¨æ–°çš„è¯´è¯äººä¿¡æ¯
                primary_speaker = segment.get("primary_speaker", "SPEAKER_UNKNOWN")
                speakers = segment.get("speakers", [primary_speaker])
                multi_speaker = segment.get("multi_speaker", False)
                speaker_confidence = segment.get("speaker_confidence", 1.0)
                
                # æå–éŸ³é¢‘ç‰‡æ®µ
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                audio_segment = y[start_sample:end_sample]
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«å¤šè¯´è¯äººæ ‡è¯†ï¼‰
                speaker_label = f"multi_{len(speakers)}" if multi_speaker else primary_speaker
                segment_path = f"./temp/professional_segment_{i+1}_{speaker_label}.wav"
                sf.write(segment_path, audio_segment, sr)
                
                # ç”Ÿæˆå¢å¼ºçš„ç‰‡æ®µä¿¡æ¯
                final_segment = {
                    "sequence": i + 1,
                    "timestamp": f"{start_time:.2f}-{end_time:.2f}",
                    "original_text": text,
                    "original_audio_path": segment_path,
                    "translated_text": "",
                    "translated_audio_path": "",
                    "voice_id": "",
                    "speed": 1.0,
                    
                    # æ–°çš„è¯´è¯äººä¿¡æ¯
                    "primary_speaker": primary_speaker,
                    "speakers": speakers,
                    "speaker_count": len(speakers),
                    "multi_speaker": multi_speaker,
                    "speaker_confidence": speaker_confidence,
                    "speaker_durations": segment.get("speaker_durations", {}),
                    
                    # å…¼å®¹æ€§å­—æ®µ
                    "speaker_id": f"speaker_{primary_speaker}",
                    
                    # è¯­ä¹‰åˆ‡åˆ†ä¿¡æ¯
                    "word_count": segment.get("word_count", 0),
                    "avg_confidence": segment.get("avg_confidence", 0.0),
                    "segment_duration": end_time - start_time
                }
                
                final_segments.append(final_segment)
                
                # è®°å½•å¤šè¯´è¯äººç‰‡æ®µ
                if multi_speaker:
                    self.logger.log("INFO", f"ğŸ¯ ç‰‡æ®µ{i+1}: å¤šè¯´è¯äºº {speakers} (ä¸»è¦: {primary_speaker}, ç½®ä¿¡åº¦: {speaker_confidence:.2f})")
            
            # ç»Ÿè®¡ä¿¡æ¯
            multi_count = sum(1 for seg in final_segments if seg["multi_speaker"])
            avg_duration = sum(seg["segment_duration"] for seg in final_segments) / len(final_segments)
            self.logger.log("INFO", f"ğŸ¯ éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå®Œæˆ: {len(final_segments)}ä¸ªç‰‡æ®µ, {multi_count}ä¸ªå¤šè¯´è¯äºº, å¹³å‡æ—¶é•¿{avg_duration:.2f}ç§’")
            
            return final_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå¤±è´¥: {str(e)}")
            return []