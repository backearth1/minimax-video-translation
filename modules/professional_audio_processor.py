import os
import torch
import torchaudio
import whisper_timestamped as whisper
from pyannote.audio import Pipeline
from typing import Dict, Any, List, Tuple
import librosa
import soundfile as sf
import subprocess
import tempfile
from pathlib import Path
from .model_manager import ModelManager

class ProfessionalAudioProcessor:
    """
    ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨
    é›†æˆ Demucs + pyannote.audio + whisper-timestamped
    æä¾›å·¥ä¸šçº§éŸ³é¢‘åˆ†ç¦»å’Œç²¾ç¡®ASRåˆ†å‰²
    """
    
    def __init__(self, logger_service):
        self.logger = logger_service
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager(logger_service)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹ - åªåœ¨å®é™…ä½¿ç”¨æ—¶åŠ è½½
        self.whisper_model = None
        self.diarization_pipeline = None
        self._models_initialized = False
        self.recommended_config = {}
        
        self.logger.log("INFO", "ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆ›å»ºå®Œæˆ (æ¨¡å‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½)")
    
    def check_models_status(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çŠ¶æ€ - ç”¨æˆ·å¯ä¸»åŠ¨è°ƒç”¨"""
        self.logger.log("INFO", "ğŸ” æ£€æŸ¥ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹çŠ¶æ€...")
        self.model_manager.print_model_status()
        
        # æä¾›ä¸‹è½½å»ºè®®
        status = self.model_manager.check_model_availability()
        missing_any = any(not info["available"] for info in status.values())
        
        if missing_any:
            self.logger.log("INFO", "ğŸ’¡ å»ºè®®:")
            self.logger.log("INFO", "   1. é¦–æ¬¡ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ¨¡å‹")
            self.logger.log("INFO", "   2. å»ºè®®åœ¨ç½‘ç»œè‰¯å¥½æ—¶è¿›è¡Œé¦–æ¬¡å¤„ç†")
            self.logger.log("INFO", "   3. æ‰€æœ‰æ¨¡å‹ä»…éœ€ä¸‹è½½ä¸€æ¬¡ï¼Œåç»­ä½¿ç”¨æ— éœ€é‡å¤ä¸‹è½½")
        else:
            self.logger.log("INFO", "âœ… æ‰€æœ‰æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç›´æ¥ä½¿ç”¨ä¸“ä¸šAIç¿»è¯‘")
    
    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶å†µ"""
        try:
            import psutil
            
            # è·å–å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            
            self.logger.log("INFO", f"ç³»ç»Ÿå¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            
            # å†…å­˜ä¸è¶³è­¦å‘Š
            if available_gb < 2:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜ä¸è¶³2GBï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            elif available_gb < 4:
                self.logger.log("WARNING", "ç³»ç»Ÿå¯ç”¨å†…å­˜è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹")
                
            return True
            
        except ImportError:
            self.logger.log("WARNING", "æ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æº (psutilæœªå®‰è£…)")
            return True
        except Exception as e:
            self.logger.log("WARNING", f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}")
            return True
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ¨¡å‹"""
        try:
            self.logger.log("INFO", "æ­£åœ¨åˆå§‹åŒ–ä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹...")
            
            # 0. æ£€æŸ¥ç³»ç»Ÿèµ„æºå’Œæ¨¡å‹å¯ç”¨æ€§
            if not self._check_system_resources():
                self.logger.log("ERROR", "ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹åˆå§‹åŒ–")
                return
            
            # è·å–æ¨èçš„æ¨¡å‹é…ç½®
            self.recommended_config = self.model_manager.prepare_models_for_professional_processing()
            
            # 1. åˆå§‹åŒ– Whisper-timestamped (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_whisper = self.recommended_config.get("whisper", "base")
                self.logger.log("INFO", f"åŠ è½½ Whisper-timestamped æ¨¡å‹: {recommended_whisper}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                status = self.model_manager.check_model_availability()
                if not status["whisper"]["available"] or recommended_whisper not in status["whisper"]["cached_models"]:
                    estimate = self.model_manager.estimate_download_time("whisper").get(recommended_whisper, "æœªçŸ¥")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ {recommended_whisper} æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # å°è¯•åŠ è½½æ¨èæ¨¡å‹ï¼Œå¤±è´¥åˆ™å›é€€
                model_priority = [recommended_whisper, "base"] if recommended_whisper != "base" else ["base"]
                
                for model_name in model_priority:
                    try:
                        self.logger.log("INFO", f"å°è¯•åŠ è½½ {model_name} æ¨¡å‹...")
                        
                        # æ·»åŠ å†…å­˜æ£€æŸ¥
                        if self.device.type == "cuda":
                            import torch
                            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                        
                        # ä½¿ç”¨é¡¹ç›®æ¨¡å‹ç›®å½•
                        project_model_dir = self.model_manager.models_dir
                        whisper_model_path = os.path.join(project_model_dir, "whisper", f"{model_name}.pt")
                        
                        if os.path.exists(whisper_model_path):
                            # ä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                            self.whisper_model = whisper.load_model(whisper_model_path, device=self.device)
                        else:
                            # å›é€€åˆ°æ ‡å‡†åŠ è½½ï¼ˆä¼šè§¦å‘ä¸‹è½½ï¼‰
                            self.whisper_model = whisper.load_model(model_name, device=self.device)
                        self.logger.log("INFO", f"âœ… Whisper {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        break
                    except Exception as model_err:
                        self.logger.log("WARNING", f"{model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {str(model_err)}")
                        # å°è¯•é‡Šæ”¾å†…å­˜
                        if hasattr(self, 'whisper_model') and self.whisper_model:
                            del self.whisper_model
                            self.whisper_model = None
                        continue
                
                if not self.whisper_model:
                    raise Exception("æ‰€æœ‰ Whisper æ¨¡å‹åŠ è½½å¤±è´¥")
                    
            except Exception as e:
                self.logger.log("ERROR", f"Whisper æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                self.whisper_model = None
            
            # 2. åˆå§‹åŒ– pyannote.audio (æ ¹æ®æ¨èé…ç½®)
            try:
                recommended_pyannote = self.recommended_config.get("pyannote", "pyannote/speaker-diarization-3.1")
                self.logger.log("INFO", f"åŠ è½½ pyannote.audio æ¨¡å‹: {recommended_pyannote}")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                if not status["pyannote"]["available"]:
                    estimate = self.model_manager.estimate_download_time("pyannote").get(recommended_pyannote, "2-3åˆ†é’Ÿ")
                    self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä¸‹è½½ pyannote.audio æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
                
                # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­çš„HuggingFace token
                auth_token = os.getenv("HUGGINGFACE_TOKEN", None)
                
                # è®¾ç½®HF_HOMEæŒ‡å‘é¡¹ç›®ç›®å½•ï¼Œè®©pyannoteä»é¡¹ç›®ç›®å½•åŠ è½½æ¨¡å‹
                old_hf_home = os.environ.get("HF_HOME", None)
                os.environ["HF_HOME"] = os.path.join(self.model_manager.models_dir, "pyannote")
                
                try:
                    # ä½¿ç”¨æ ‡å‡†æ¨¡å‹ååŠ è½½ï¼ˆä¼šä»HF_HOMEæŸ¥æ‰¾ï¼‰
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        recommended_pyannote,
                        use_auth_token=auth_token
                    )
                    
                    self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                    self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as load_err:
                    self.logger.log("WARNING", f"pyannote.audio æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_err)}")
                    # å¦‚æœtokenæœ‰é—®é¢˜ï¼Œå°è¯•æ— tokenåŠ è½½
                    if "token" in str(load_err).lower() or "unauthorized" in str(load_err).lower():
                        self.logger.log("INFO", "å°è¯•æ— tokenåŠ è½½pyannote.audio...")
                        try:
                            self.diarization_pipeline = Pipeline.from_pretrained(recommended_pyannote)
                            self.diarization_pipeline = self.diarization_pipeline.to(self.device)
                            self.logger.log("INFO", "âœ… pyannote.audio æ¨¡å‹åŠ è½½æˆåŠŸ(æ— token)")
                        except Exception as e2:
                            self.logger.log("ERROR", f"æ— tokenåŠ è½½ä¹Ÿå¤±è´¥: {str(e2)}")
                            raise load_err
                finally:
                    # æ¢å¤åŸå§‹HF_HOMEç¯å¢ƒå˜é‡
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
                    
            except Exception as e:
                self.logger.log("ERROR", f"pyannote.audio åŠ è½½å¤±è´¥: {str(e)}")
                self.diarization_pipeline = None
                
                # ç¡®ä¿æ¢å¤ç¯å¢ƒå˜é‡
                if 'old_hf_home' in locals():
                    if old_hf_home is not None:
                        os.environ["HF_HOME"] = old_hf_home
                    elif "HF_HOME" in os.environ:
                        del os.environ["HF_HOME"]
            
            self.logger.log("INFO", f"ğŸš€ ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
            
        except Exception as e:
            self.logger.log("ERROR", f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def process_audio_professionally(self, audio_path: str, source_language: str = "zh", project_data=None) -> Dict[str, Any]:
        """
        ä¸“ä¸šéŸ³é¢‘å¤„ç†ä¸»æµç¨‹
        
        Args:
            audio_path: åŸå§‹éŸ³é¢‘è·¯å¾„
            source_language: æºè¯­è¨€ä»£ç 
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            self.logger.log("INFO", "ğŸµ å¼€å§‹ä¸“ä¸šéŸ³é¢‘å¤„ç†æµç¨‹...")
            
            # é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–æ¨¡å‹
            if not self._models_initialized:
                self._initialize_models()
                self._models_initialized = True
            
            # æ­¥éª¤1: Demucs éŸ³é¢‘æºåˆ†ç¦»
            separation_result = self._separate_audio_sources(audio_path)
            if not separation_result["success"]:
                return separation_result
            
            vocals_path = separation_result["vocals_path"]
            background_path = separation_result["background_path"]
            
            # ç«‹å³æ›´æ–°project_dataä»¥ä¾¿å‰ç«¯é¢„è§ˆ
            if project_data:
                project_data.vocals_audio_path = vocals_path
                project_data.background_audio_path = background_path
                project_data.set_processing_status("processing", "ğŸµ éŸ³é¢‘åˆ†ç¦»å®Œæˆï¼Œå¼€å§‹è¯´è¯äººåˆ†æ...", 30)
                self.logger.log("INFO", "âœ… Demucsåˆ†ç¦»å®Œæˆï¼ŒéŸ³é¢‘é¢„è§ˆå·²æ›´æ–°")
            
            # æ­¥éª¤2: pyannote.audio è¯´è¯äººåˆ†ç¦»
            self.logger.log("INFO", "ğŸ“Š å¼€å§‹è¯´è¯äººåˆ†ç¦»åˆ†æ...")
            speaker_segments = self._analyze_speakers(vocals_path)
            
            # æ­¥éª¤3: whisper-timestamped å­—çº§åˆ«ASR
            self.logger.log("INFO", "ğŸ“ å¼€å§‹å­—çº§åˆ«è¯­éŸ³è¯†åˆ«...")
            word_timestamps = self._transcribe_with_timestamps(vocals_path, source_language)
            
            # æ­¥éª¤4: ç»“æœå¯¹é½å’Œåˆ†ç»„
            self.logger.log("INFO", "ğŸ”— å¼€å§‹è¯´è¯äººä¸æ–‡å­—å¯¹é½...")
            aligned_segments = self._align_speakers_with_words(speaker_segments, word_timestamps)
            
            # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç‰‡æ®µ
            final_segments = self._generate_audio_segments(vocals_path, aligned_segments)
            
            self.logger.log("INFO", f"âœ… ä¸“ä¸šéŸ³é¢‘å¤„ç†å®Œæˆ: {len(final_segments)}ä¸ªç²¾ç¡®ç‰‡æ®µ")
            
            return {
                "success": True,
                "vocals_path": vocals_path,
                "background_path": background_path,
                "segments": final_segments,
                "total_segments": len(final_segments)
            }
            
        except Exception as e:
            error_msg = f"ä¸“ä¸šéŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}"
            self.logger.log("ERROR", error_msg)
            return {"success": False, "error": error_msg}
    
    def _separate_audio_sources(self, audio_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»"""
        try:
            self.logger.log("INFO", "ğŸ¼ ä½¿ç”¨ Demucs è¿›è¡ŒéŸ³é¢‘æºåˆ†ç¦»...")
            
            # æ£€æŸ¥Demucsæ¨¡å‹çŠ¶æ€
            status = self.model_manager.check_model_availability()
            if not status["demucs"]["available"]:
                estimate = self.model_manager.estimate_download_time("demucs").get("htdemucs", "3-5åˆ†é’Ÿ")
                self.logger.log("INFO", f"ğŸŒ é¦–æ¬¡ä½¿ç”¨ Demucsï¼Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œé¢„è®¡è€—æ—¶: {estimate}")
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = "./temp/demucs_output"
            os.makedirs(output_dir, exist_ok=True)
            
            # è¿è¡Œ Demucs åˆ†ç¦» (ä½¿ç”¨UVç¯å¢ƒï¼ŒæŒ‡å®šé¡¹ç›®æ¨¡å‹)
            # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å‘é¡¹ç›®æ¨¡å‹ç›®å½•
            env = os.environ.copy()
            env["TORCH_HOME"] = os.path.join(self.model_manager.models_dir, "demucs")
            
            cmd = [
                "uv", "run", "python", "-m", "demucs.separate",
                "-n", "htdemucs",  # ä½¿ç”¨é«˜è´¨é‡htdemucsæ¨¡å‹
                "--mp3",  # è¾“å‡ºMP3æ ¼å¼
                "--mp3-bitrate", "320",  # é«˜è´¨é‡
                "-o", output_dir,
                audio_path
            ]
            
            # å¯¹äº32ç§’éŸ³é¢‘ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’(10åˆ†é’Ÿ)
            self.logger.log("INFO", f"æ‰§è¡ŒDemucså‘½ä»¤: {' '.join(cmd)}")
            self.logger.log("INFO", f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {env['TORCH_HOME']}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=env)
            
            if result.returncode != 0:
                return {"success": False, "error": f"Demucs åˆ†ç¦»å¤±è´¥: {result.stderr}"}
            
            # æŸ¥æ‰¾åˆ†ç¦»åçš„æ–‡ä»¶
            audio_name = Path(audio_path).stem
            demucs_subdir = os.path.join(output_dir, "htdemucs", audio_name)
            
            vocals_path = os.path.join(demucs_subdir, "vocals.mp3")
            background_paths = [
                os.path.join(demucs_subdir, "drums.mp3"),
                os.path.join(demucs_subdir, "bass.mp3"), 
                os.path.join(demucs_subdir, "other.mp3")
            ]
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(vocals_path):
                return {"success": False, "error": "Demucs äººå£°åˆ†ç¦»æ–‡ä»¶æœªç”Ÿæˆ"}
            
            # åˆå¹¶éäººå£°éƒ¨åˆ†ä½œä¸ºèƒŒæ™¯éŸ³
            background_path = os.path.join(output_dir, f"{audio_name}_background.wav")
            background_success = self._merge_background_tracks(background_paths, background_path)
            
            if not background_success:
                self.logger.log("WARNING", "èƒŒæ™¯éŸ³è½¨åˆå¹¶å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºèƒŒæ™¯éŸ³")
                background_path = None
            
            # è½¬æ¢äººå£°ä¸ºWAVæ ¼å¼ç”¨äºåç»­å¤„ç†
            vocals_wav_path = os.path.join(output_dir, f"{audio_name}_vocals.wav")
            self._convert_to_wav(vocals_path, vocals_wav_path)
            
            self.logger.log("INFO", f"âœ… Demucs åˆ†ç¦»å®Œæˆ")
            self.logger.log("INFO", f"   äººå£°: {vocals_wav_path}")
            self.logger.log("INFO", f"   èƒŒæ™¯: {background_path}")
            
            return {
                "success": True,
                "vocals_path": vocals_wav_path,
                "background_path": background_path
            }
            
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Demucs å¤„ç†è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "error": f"Demucs å¤„ç†å¼‚å¸¸: {str(e)}"}
    
    def _merge_background_tracks(self, track_paths: List[str], output_path: str) -> bool:
        """åˆå¹¶èƒŒæ™¯éŸ³è½¨ï¼ˆdrums + bass + otherï¼‰"""
        try:
            existing_tracks = [p for p in track_paths if os.path.exists(p)]
            self.logger.log("INFO", f"æ‰¾åˆ°{len(existing_tracks)}ä¸ªèƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
            
            if not existing_tracks:
                self.logger.log("WARNING", "æ²¡æœ‰æ‰¾åˆ°èƒŒæ™¯éŸ³è½¨æ–‡ä»¶")
                return False
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä½¿ç”¨ ffmpeg æ··åˆæ‰€æœ‰èƒŒæ™¯éŸ³è½¨
            if len(existing_tracks) == 1:
                # åªæœ‰ä¸€ä¸ªè½¨é“ï¼Œç›´æ¥è½¬æ¢æ ¼å¼
                cmd = ["ffmpeg", "-i", existing_tracks[0], "-ac", "2", "-ar", "44100", "-y", output_path]
                self.logger.log("INFO", f"å•è½¨é“è½¬æ¢: {existing_tracks[0]}")
            else:
                # å¤šä¸ªè½¨é“ï¼Œéœ€è¦æ··åˆ
                input_args = []
                for track in existing_tracks:
                    input_args.extend(["-i", track])
                    self.logger.log("INFO", f"æ·»åŠ èƒŒæ™¯éŸ³è½¨: {track}")
                
                # ä¿®å¤filter_complexè¯­æ³•
                filter_inputs = "".join([f"[{i}:a]" for i in range(len(existing_tracks))])
                filter_complex = f"{filter_inputs}amix=inputs={len(existing_tracks)}:duration=longest"
                
                cmd = ["ffmpeg"] + input_args + [
                    "-filter_complex", filter_complex,
                    "-ac", "2", "-ar", "44100",
                    "-y", output_path
                ]
            
            self.logger.log("INFO", f"æ‰§è¡Œffmpegå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, timeout=120, text=True)
            
            if result.returncode == 0:
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    self.logger.log("INFO", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶æˆåŠŸ: {output_path} ({file_size} bytes)")
                    return True
                else:
                    self.logger.log("ERROR", "ffmpegæˆåŠŸä½†èƒŒæ™¯éŸ³æ–‡ä»¶æœªç”Ÿæˆ")
                    return False
            else:
                self.logger.log("ERROR", f"ffmpegå¤±è´¥ (è¿”å›ç : {result.returncode})")
                self.logger.log("ERROR", f"stderr: {result.stderr}")
                self.logger.log("ERROR", f"stdout: {result.stdout}")
                return False
            
        except subprocess.TimeoutExpired:
            self.logger.log("ERROR", "èƒŒæ™¯éŸ³è½¨åˆå¹¶è¶…æ—¶")
            return False
        except Exception as e:
            self.logger.log("ERROR", f"èƒŒæ™¯éŸ³è½¨åˆå¹¶å¼‚å¸¸: {str(e)}")
            return False
    
    def _convert_to_wav(self, input_path: str, output_path: str):
        """è½¬æ¢éŸ³é¢‘ä¸ºWAVæ ¼å¼"""
        try:
            cmd = [
                "ffmpeg", "-i", input_path,
                "-acodec", "pcm_s16le",
                "-ar", "16000", "-ac", "1",
                "-y", output_path
            ]
            subprocess.run(cmd, capture_output=True, timeout=60)
        except Exception as e:
            self.logger.log("WARNING", f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
    
    def _analyze_speakers(self, vocals_path: str) -> List[Dict]:
        """ä½¿ç”¨ pyannote.audio åˆ†æè¯´è¯äºº"""
        try:
            if not self.diarization_pipeline:
                return []
            
            diarization = self.diarization_pipeline(vocals_path)
            
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker,
                    "duration": turn.end - turn.start
                })
            
            self.logger.log("INFO", f"æ£€æµ‹åˆ° {len(set(seg['speaker'] for seg in speaker_segments))} ä¸ªè¯´è¯äºº")
            return speaker_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººåˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def _transcribe_with_timestamps(self, vocals_path: str, language: str) -> Dict:
        """ä½¿ç”¨ whisper-timestamped è¿›è¡Œå­—çº§åˆ«è½¬å½•"""
        try:
            if not self.whisper_model:
                return {}
            
            # æ˜ å°„è¯­è¨€ä»£ç 
            lang_map = {
                "ä¸­æ–‡": "zh", "è‹±è¯­": "en", "æ—¥è¯­": "ja", "éŸ©è¯­": "ko",
                "æ³•è¯­": "fr", "å¾·è¯­": "de", "è¥¿ç­ç‰™è¯­": "es"
            }
            whisper_lang = lang_map.get(language, "zh")
            
            result = whisper.transcribe(
                self.whisper_model,
                vocals_path,
                language=whisper_lang,
                vad=True,  # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼Œå‡å°‘å¹»è§‰
                compute_word_confidence=True,  # è®¡ç®—è¯æ±‡ç½®ä¿¡åº¦
                refine_whisper_precision=0.5,  # ä¼˜åŒ–æ—¶é—´æˆ³ç²¾åº¦åˆ°0.5ç§’
                min_word_duration=0.02,  # æœ€å°è¯æ±‡æŒç»­æ—¶é—´20ms
                remove_empty_words=True,  # ç§»é™¤å¯èƒ½çš„å¹»è§‰ç©ºè¯
                detect_disfluencies=False,  # æš‚æ—¶å…³é—­ä¸æµç•…æ£€æµ‹
                trust_whisper_timestamps=True  # ä¿¡ä»»Whisperçš„æ—¶é—´æˆ³ä½œä¸ºåŸºç¡€
            )
            
            self.logger.log("INFO", f"Whisper è¯†åˆ«å®Œæˆ: {len(result.get('segments', []))} ä¸ªæ®µè½")
            return result
            
        except Exception as e:
            self.logger.log("ERROR", f"Whisper è½¬å½•å¤±è´¥: {str(e)}")
            return {}
    
    def _align_speakers_with_words(self, speaker_segments: List[Dict], word_result: Dict) -> List[Dict]:
        """å°†è¯´è¯äººä¿¡æ¯ä¸æ–‡å­—æ—¶é—´æˆ³å¯¹é½"""
        try:
            aligned_segments = []
            
            # è°ƒè¯•ä¿¡æ¯
            self.logger.log("DEBUG", f"è¯´è¯äººç‰‡æ®µæ•°é‡: {len(speaker_segments)}")
            self.logger.log("DEBUG", f"Whisperç»“æœåŒ…å«segments: {'segments' in word_result if word_result else False}")
            
            if not word_result or "segments" not in word_result:
                self.logger.log("WARNING", "Whisperç»“æœä¸ºç©ºæˆ–æ²¡æœ‰segmentså­—æ®µ")
                return []
            
            # ç»Ÿè®¡è¯æ±‡æ•°é‡
            total_words = 0
            for segment in word_result["segments"]:
                if "words" in segment:
                    total_words += len(segment["words"])
            
            self.logger.log("DEBUG", f"Whisperè¯†åˆ«å‡ºæ€»è¯æ±‡æ•°: {total_words}")
            
            for i, segment in enumerate(word_result["segments"]):
                if "words" not in segment:
                    continue
                
                for word_info in segment["words"]:
                    word_start = word_info.get("start", 0)
                    word_end = word_info.get("end", 0)
                    word_text = word_info.get("text", "").strip()  # ä¿®å¤ï¼šä½¿ç”¨'text'è€Œä¸æ˜¯'word'
                    
                    if not word_text:
                        continue
                    
                    # æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº
                    speaker = self._find_speaker_at_time(speaker_segments, word_start, word_end)
                    
                    aligned_segments.append({
                        "start": word_start,
                        "end": word_end,
                        "text": word_text,
                        "speaker": speaker
                    })
            
            self.logger.log("DEBUG", f"å¯¹é½åè¯æ±‡æ•°é‡: {len(aligned_segments)}")
            
            # å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­
            grouped_segments = self._group_consecutive_words(aligned_segments)
            
            self.logger.log("INFO", f"å¯¹é½å®Œæˆ: {len(grouped_segments)} ä¸ªè¯´è¯äººç‰‡æ®µ")
            return grouped_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"è¯´è¯äººæ–‡å­—å¯¹é½å¤±è´¥: {str(e)}")
            return []
    
    def _find_speaker_at_time(self, speaker_segments: List[Dict], start_time: float, end_time: float) -> str:
        """æ ¹æ®æ—¶é—´æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº"""
        word_center = (start_time + end_time) / 2
        
        for segment in speaker_segments:
            if segment["start"] <= word_center <= segment["end"]:
                return segment["speaker"]
        
        # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œæ‰¾æœ€è¿‘çš„
        closest_speaker = "SPEAKER_UNKNOWN"
        min_distance = float('inf')
        
        for segment in speaker_segments:
            seg_center = (segment["start"] + segment["end"]) / 2
            distance = abs(word_center - seg_center)
            if distance < min_distance:
                min_distance = distance
                closest_speaker = segment["speaker"]
        
        return closest_speaker
    
    def _group_consecutive_words(self, word_segments: List[Dict]) -> List[Dict]:
        """å°†è¿ç»­çš„ç›¸åŒè¯´è¯äººçš„è¯ç»„åˆæˆå¥å­"""
        if not word_segments:
            return []
        
        grouped = []
        current_group = {
            "start": word_segments[0]["start"],
            "end": word_segments[0]["end"],
            "text": word_segments[0]["text"],
            "speaker": word_segments[0]["speaker"]
        }
        
        for i in range(1, len(word_segments)):
            word = word_segments[i]
            
            # å¦‚æœæ˜¯ç›¸åŒè¯´è¯äººä¸”æ—¶é—´è¿ç»­ï¼ˆé—´éš”<2ç§’ï¼‰ï¼Œåˆ™åˆå¹¶
            if (word["speaker"] == current_group["speaker"] and 
                word["start"] - current_group["end"] < 2.0):
                
                current_group["end"] = word["end"]
                current_group["text"] += word["text"]
            else:
                # ä¿å­˜å½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„
                grouped.append(current_group.copy())
                current_group = {
                    "start": word["start"],
                    "end": word["end"], 
                    "text": word["text"],
                    "speaker": word["speaker"]
                }
        
        # æ·»åŠ æœ€åä¸€ç»„
        grouped.append(current_group)
        
        return grouped
    
    def _generate_audio_segments(self, vocals_path: str, aligned_segments: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆæœ€ç»ˆçš„éŸ³é¢‘ç‰‡æ®µ"""
        try:
            final_segments = []
            
            # åŠ è½½äººå£°éŸ³é¢‘
            y, sr = librosa.load(vocals_path, sr=16000)
            
            for i, segment in enumerate(aligned_segments):
                start_time = segment["start"]
                end_time = segment["end"]
                text = segment["text"].strip()
                speaker = segment["speaker"]
                
                # æå–éŸ³é¢‘ç‰‡æ®µ
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                audio_segment = y[start_sample:end_sample]
                
                # ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
                segment_path = f"./temp/professional_segment_{i+1}.wav"
                sf.write(segment_path, audio_segment, sr)
                
                final_segments.append({
                    "sequence": i + 1,
                    "timestamp": f"{start_time:.2f}-{end_time:.2f}",
                    "original_text": text,
                    "original_audio_path": segment_path,
                    "speaker_id": f"speaker_{speaker}",
                    "translated_text": "",
                    "translated_audio_path": "",
                    "voice_id": "",
                    "speed": 1.0
                })
            
            return final_segments
            
        except Exception as e:
            self.logger.log("ERROR", f"éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå¤±è´¥: {str(e)}")
            return []